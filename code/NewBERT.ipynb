{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10258418,"sourceType":"datasetVersion","datasetId":6345928},{"sourceId":10258423,"sourceType":"datasetVersion","datasetId":6345932},{"sourceId":10262339,"sourceType":"datasetVersion","datasetId":6348456},{"sourceId":10264197,"sourceType":"datasetVersion","datasetId":6349883}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importation des bibliothèques nécessaires pour le traitement du langage naturel et l'apprentissage automatique\nimport nltk\nimport os\nimport csv\nimport math\nimport random\nimport logging\nimport itertools\nimport time\nimport re\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    roc_auc_score,\n    balanced_accuracy_score,\n    matthews_corrcoef\n)\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import (\n    BertModel, BertConfig, BertPreTrainedModel,\n    BertTokenizer, AdamW, get_linear_schedule_with_warmup\n)\nfrom nltk.corpus import wordnet as wn\nfrom torch.nn.functional import softmax\nfrom tabulate import tabulate\n\n# Configuration de la journalisation pour un suivi détaillé des opérations\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n#Téléchargement des ressources WordNet nécessaires\nnltk.download('wordnet', download_dir='/kaggle/working/nltk_data')\nnltk.data.path.append('/kaggle/working/nltk_data')","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:25:35.082140Z","iopub.execute_input":"2024-12-21T16:25:35.082437Z","iopub.status.idle":"2024-12-21T16:25:40.819073Z","shell.execute_reply.started":"2024-12-21T16:25:35.082409Z","shell.execute_reply":"2024-12-21T16:25:40.818339Z"},"id":"etBj-YbIm87k","outputId":"be9c35d1-bce0-411c-fab6-a1a3a2b9127e","trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#Google Colab Unzipping Methods\n#Kaggle Notebook Unzipping Methods\n#Python's zipfile module\n\nimport zipfile\n\nwith zipfile.ZipFile('/kaggle/working/nltk_data/corpora/wordnet.zip', 'r') as zip_ref:\n    zip_ref.extractall('//kaggle/working/nltk_data/corpora/')","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:25:46.705973Z","iopub.execute_input":"2024-12-21T16:25:46.706401Z","iopub.status.idle":"2024-12-21T16:25:46.932587Z","shell.execute_reply.started":"2024-12-21T16:25:46.706376Z","shell.execute_reply":"2024-12-21T16:25:46.931698Z"},"id":"qXCtVC6A8OOx","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Structure représentant un enregistrement pour la sélection de gloses\nfrom collections import namedtuple\nGlossSelectionRecord = namedtuple(\"GlossSelectionRecord\", [\"identifiant\", \"phrase\", \"cles_sens\", \"glosses\", \"cibles\"])\nEntreesBert = namedtuple(\"EntreesBert\", [\"ids_entree\", \"masque_entree\", \"ids_segment\", \"id_etiquette\"])\n\n# Définition des parties du discours pour WordNet\nPARTIES_DISCOURS_WORDNET = {'VERBE': wn.VERB, 'NOM': wn.NOUN, 'ADJECTIF': wn.ADJ, 'ADVERBE': wn.ADV}\n\ndef obtenir_glosses(lemme, pos):\n    \"\"\"\n    Récupère les définitions pour un mot donné dans WordNet.\n\n    L'utilisateur fournit un lemme et une partie du discours,\n    la fonction renvoie un dictionnaire de définitions.\n    \"\"\"\n    resultats = dict()\n    pos_wordnet = PARTIES_DISCOURS_WORDNET.get(pos, None) if pos is not None else None\n    morphemes = wn._morphy(lemme, pos=pos_wordnet) if pos is not None else []\n\n    for synset in set(wn.synsets(lemme, pos=pos_wordnet)):\n        cle_sens = None\n        for lemme_synset in synset.lemmas():\n            if lemme_synset.name().lower() == lemme.lower():\n                cle_sens = lemme_synset.key()\n                break\n            elif lemme_synset.name().lower() in morphemes:\n                cle_sens = lemme_synset.key()\n\n        if cle_sens is not None:\n            resultats[cle_sens] = synset.definition()\n\n    return resultats\n\nclass JeuDonneesWSD(Dataset):\n    \"\"\"Jeu de données personnalisé pour la désambiguïsation lexicale\"\"\"\n    def __init__(self, caracteristiques):\n        self.caracteristiques = caracteristiques\n\n    def __getitem__(self, index):\n        return self.caracteristiques[index]\n\n    def __len__(self):\n        return len(self.caracteristiques)","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:25:53.169183Z","iopub.execute_input":"2024-12-21T16:25:53.169458Z","iopub.status.idle":"2024-12-21T16:25:54.556187Z","shell.execute_reply.started":"2024-12-21T16:25:53.169437Z","shell.execute_reply":"2024-12-21T16:25:54.555456Z"},"id":"bObSAtLatKoc","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class BertWSD(BertPreTrainedModel):\n    \"\"\"\n    Modèle BERT spécialisé pour la désambiguïsation lexicale.\n\n    Le modèle étend BertPreTrainedModel avec une couche de classification linéaire.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n        self.couche_classement = torch.nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n\ndef calculer_perte_ponderee(perte, facteur_ponderation):\n    \"\"\"\n    Calcule une perte pondérée avec un facteur de correction.\n\n    Permet d'ajuster la contribution de la perte lors de l'entraînement.\n    \"\"\"\n    carre_facteur = facteur_ponderation ** 2\n    return 1 / (2 * carre_facteur) * perte + math.log(1 + carre_facteur)\n\ndef tronquer_sequence_paire(jetons_a, jetons_b, longueur_max):\n    \"\"\"\n    Tronque une paire de séquences à une longueur maximale.\n\n    Assure que la longueur totale des séquences ne dépasse pas la limite.\n    \"\"\"\n    while True:\n        longueur_totale = len(jetons_a) + len(jetons_b)\n        if longueur_totale <= longueur_max:\n            break\n        if len(jetons_a) > len(jetons_b):\n            jetons_a.pop()\n        else:\n            jetons_b.pop()","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:26:02.569264Z","iopub.execute_input":"2024-12-21T16:26:02.569548Z","iopub.status.idle":"2024-12-21T16:26:02.575353Z","shell.execute_reply.started":"2024-12-21T16:26:02.569527Z","shell.execute_reply":"2024-12-21T16:26:02.574678Z"},"id":"vwblPaVRm87m","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Tuple\n\n@dataclass\nclass ConfigurationEchantillonnage:\n    \"\"\"Configuration pour l'échantillonnage des données.\n    \n    Attributes:\n        taille_max: Nombre maximum d'échantillons à prendre\n        plage: Tuple optionnel (debut, fin) pour spécifier une plage de lignes\n    \"\"\"\n    taille_max: Optional[int] = None\n    plage: Optional[Tuple[int, int]] = None\n\ndef creer_enregistrements_depuis_csv(chemin_csv, fonction_deserialisation, config_echantillonnage=None):\n    \"\"\"\n    Crée des enregistrements à partir d'un fichier CSV avec options d'échantillonnage avancées.\n    \n    Args:\n        chemin_csv: Chemin vers le fichier CSV\n        fonction_deserialisation: Fonction pour convertir les lignes CSV\n        config_echantillonnage: Configuration d'échantillonnage (taille_max et plage)\n        \n    Returns:\n        Liste des enregistrements échantillonnés\n    \"\"\"\n    enregistrements = []\n    \n    with open(chemin_csv, 'r', encoding='utf-8', newline='') as fichier:\n        lecteur = csv.reader(fichier)\n        next(lecteur)  # Ignorer l'en-tête\n        \n        # Convertir l'itérateur en liste pour permettre l'indexation\n        toutes_lignes = list(lecteur)\n        \n        # Déterminer les indices de début et fin\n        debut = 0\n        fin = len(toutes_lignes)\n        \n        if config_echantillonnage and config_echantillonnage.plage:\n            debut = max(0, config_echantillonnage.plage[0] - 1)  # -1 car les utilisateurs comptent à partir de 1\n            fin = min(len(toutes_lignes), config_echantillonnage.plage[1])\n        \n        # Appliquer la limitation de taille si spécifiée\n        if config_echantillonnage and config_echantillonnage.taille_max:\n            fin = min(fin, debut + config_echantillonnage.taille_max)\n        \n        # Créer les enregistrements pour la plage sélectionnée\n        for ligne in toutes_lignes[debut:fin]:\n            enregistrements.append(fonction_deserialisation(ligne))\n            \n        logger.info(f\"Échantillonnage effectué - Plage : {debut+1} à {fin} - \"\n                   f\"Nombre d'enregistrements : {len(enregistrements)}\")\n    \n    return enregistrements\n\ndef deserialiser_enregistrement_csv(ligne):\n    \"\"\"\n    Convertit une ligne CSV en enregistrement de sélection de gloses.\n    \"\"\"\n    return GlossSelectionRecord(\n        ligne[0],  # identifiant\n        ligne[1],  # phrase\n        eval(ligne[2]),  # cles_sens\n        eval(ligne[3]),  # glosses\n        [int(t) for t in eval(ligne[4])]  # cibles\n    )\n\ndef charger_jeu_donnees(\n    chemin_csv,\n    tokeniseur,\n    longueur_sequence_max,\n    debut_plage=None,\n    fin_plage=None,\n    taille_max=None\n):\n    \"\"\"\n    Charge un jeu de données avec options d'échantillonnage avancées.\n    \n    Args:\n        chemin_csv: Chemin vers le fichier CSV\n        tokeniseur: Tokeniseur BERT\n        longueur_sequence_max: Longueur maximale des séquences\n        debut_plage: Indice de début pour l'échantillonnage (commençant à 1)\n        fin_plage: Indice de fin pour l'échantillonnage\n        taille_max: Nombre maximum d'échantillons à prendre\n    \"\"\"\n    config = ConfigurationEchantillonnage(\n        taille_max=taille_max,\n        plage=(debut_plage, fin_plage) if debut_plage and fin_plage else None\n    )\n    \n    enregistrements = creer_enregistrements_depuis_csv(\n        chemin_csv,\n        deserialiser_enregistrement_csv,\n        config\n    )\n    \n    caracteristiques = creer_caracteristiques_depuis_enregistrements(\n        enregistrements,\n        longueur_sequence_max,\n        tokeniseur\n    )\n    \n    logger.info(f\"Chargé {len(caracteristiques)} échantillons depuis {chemin_csv}\")\n    \n    return JeuDonneesWSD(caracteristiques)\n\ndef creer_caracteristiques_depuis_enregistrements(enregistrements, longueur_seq_max, tokeniseur):\n    \"\"\"\n    Convertit les enregistrements en caractéristiques pour BERT.\n\n    Prépare les données d'entrée pour le modèle en tokenisant et formatant.\n    \"\"\"\n    caracteristiques = []\n    for enregistrement in tqdm(enregistrements, desc=\"Conversion des données\"):\n        jetons_a = tokeniseur.tokenize(enregistrement.phrase)\n        sequences = [(gloss, 1 if i in enregistrement.cibles else 0) for i, gloss in enumerate(enregistrement.glosses)]\n\n        paires = []\n        for seq, etiquette in sequences:\n            jetons_b = tokeniseur.tokenize(seq)\n            tronquer_sequence_paire(jetons_a, jetons_b, longueur_seq_max - 3)\n\n            jetons = jetons_a + ['[SEP]']\n            ids_segment = [0] * len(jetons)\n\n            jetons += jetons_b + ['[SEP]']\n            ids_segment += [1] * (len(jetons_b) + 1)\n\n            jetons = ['[CLS]'] + jetons\n            ids_segment = [0] + ids_segment\n\n            ids_entree = tokeniseur.convert_tokens_to_ids(jetons)\n            masque_entree = [1] * len(ids_entree)\n\n            longueur_padding = longueur_seq_max - len(ids_entree)\n            ids_entree += [0] * longueur_padding\n            masque_entree += [0] * longueur_padding\n            ids_segment += [0] * longueur_padding\n\n            assert len(ids_entree) == longueur_seq_max\n            assert len(masque_entree) == longueur_seq_max\n            assert len(ids_segment) == longueur_seq_max\n\n            paires.append(\n                EntreesBert(ids_entree=ids_entree, masque_entree=masque_entree,\n                            ids_segment=ids_segment, id_etiquette=etiquette)\n            )\n\n        caracteristiques.append(paires)\n\n    return caracteristiques\n\ndef regrouper_lots(lot):\n    \"\"\"\n    Regroupe les lots de données pour l'entraînement et l'évaluation.\n\n    Prépare les tenseurs pour l'entrée du modèle BERT.\n    \"\"\"\n    longueur_seq_max = len(lot[0][0].ids_entree)\n\n    lots_regroupes = []\n    for sous_lot in lot:\n        taille_lot = len(sous_lot)\n        sous_lots_regroupes = [torch.zeros([taille_lot, longueur_seq_max], dtype=torch.long) for _ in range(3)] + \\\n                               [torch.zeros([taille_lot], dtype=torch.long)]\n\n        for i, entree_bert in enumerate(sous_lot):\n            sous_lots_regroupes[0][i] = torch.tensor(entree_bert.ids_entree, dtype=torch.long)\n            sous_lots_regroupes[1][i] = torch.tensor(entree_bert.masque_entree, dtype=torch.long)\n            sous_lots_regroupes[2][i] = torch.tensor(entree_bert.ids_segment, dtype=torch.long)\n            sous_lots_regroupes[3][i] = torch.tensor(entree_bert.id_etiquette, dtype=torch.long)\n\n        lots_regroupes.append(sous_lots_regroupes)\n\n    return lots_regroupes","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:26:17.599403Z","iopub.execute_input":"2024-12-21T16:26:17.599698Z","iopub.status.idle":"2024-12-21T16:26:17.614967Z","shell.execute_reply.started":"2024-12-21T16:26:17.599674Z","shell.execute_reply":"2024-12-21T16:26:17.614099Z"},"id":"iKSduMGFm87n","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def passe_avant_selection_gloses(modele, lots, appareil):\n    \"\"\"\n    Réalise une passe avant pour la sélection de gloses.\n\n    Le système calcule la perte et les logits pour chaque lot de données\n    en utilisant le modèle BERT spécialisé.\n    \"\"\"\n    perte_lot = 0\n    liste_logits = []\n    fonction_perte = torch.nn.CrossEntropyLoss()\n\n    for lot in lots:\n        lot = tuple(t.to(appareil) for t in lot)\n        sorties = modele.bert(input_ids=lot[0], attention_mask=lot[1], token_type_ids=lot[2])\n        etat_cache = modele.dropout(sorties[1])\n\n        logits = modele.couche_classement(etat_cache).squeeze(-1)\n        etiquettes = torch.max(lot[3], -1).indices.detach()\n        perte_lot += fonction_perte(logits.unsqueeze(dim=0), etiquettes.unsqueeze(dim=-1))\n        liste_logits.append(logits)\n\n    perte = perte_lot / len(lots)\n    return perte, liste_logits\n\ndef entrainer_wsd(\n    chemin_entrainement,\n    chemin_evaluation,\n    repertoire_sortie= '/kaggle/working/resultats',\n    debut_plage_entrainement=None,\n    fin_plage_entrainement=None,\n    taille_max_entrainement=None,\n    debut_plage_evaluation=None,\n    fin_plage_evaluation=None,\n    taille_max_evaluation=None\n):\n    \"\"\"\n    Entraîne un modèle de désambiguïsation lexicale avec options d'échantillonnage avancées.\n    \n    Args:\n        chemin_entrainement: Chemin vers les données d'entraînement\n        chemin_evaluation: Chemin vers les données d'évaluation\n        repertoire_sortie: Répertoire pour sauvegarder le modèle\n        debut_plage_entrainement: Première ligne à inclure pour l'entraînement\n        fin_plage_entrainement: Dernière ligne à inclure pour l'entraînement\n        taille_max_entrainement: Nombre maximum d'échantillons d'entraînement\n        debut_plage_evaluation: Première ligne à inclure pour l'évaluation\n        fin_plage_evaluation: Dernière ligne à inclure pour l'évaluation\n        taille_max_evaluation: Nombre maximum d'échantillons d'évaluation\n    \"\"\"\n    # Configuration des hyperparamètres\n    longueur_sequence_max = 128\n    taille_lot = 8\n    nombre_epoques = 1\n    taux_apprentissage = 1e-5\n    graine = 42\n\n    # Configuration de la reproductibilité\n    random.seed(graine)\n    np.random.seed(graine)\n    torch.manual_seed(graine)\n    torch.cuda.manual_seed_all(graine)\n\n    # Sélection et configuration du périphérique\n    appareil = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(f\"Utilisation du périphérique : {appareil}\")\n\n    # Chargement du modèle et du tokeniseur\n    nom_modele = '/kaggle/input/model-last'\n    configuration = BertConfig.from_pretrained(nom_modele, num_labels=2)\n    tokeniseur = BertTokenizer.from_pretrained(nom_modele)\n    modele = BertWSD.from_pretrained(nom_modele, config=configuration)\n\n    # Gestion des tokens spéciaux\n    if '[TGT]' not in tokeniseur.additional_special_tokens:\n        tokeniseur.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n        modele.resize_token_embeddings(len(tokeniseur))\n\n    modele.to(appareil)\n\n    # Chargement des données d'entraînement avec les nouvelles options d'échantillonnage\n    logger.info(\"Chargement des données d'entraînement...\")\n    jeu_donnees_entrainement = charger_jeu_donnees(\n        chemin_entrainement,\n        tokeniseur,\n        longueur_sequence_max,\n        debut_plage=debut_plage_entrainement,\n        fin_plage=fin_plage_entrainement,\n        taille_max=taille_max_entrainement\n    )\n\n    # Création du DataLoader pour l'entraînement\n    echantillonneur = RandomSampler(jeu_donnees_entrainement)\n    chargeur_donnees_entrainement = DataLoader(\n        jeu_donnees_entrainement,\n        sampler=echantillonneur,\n        batch_size=taille_lot,\n        collate_fn=regrouper_lots\n    )\n\n    # Configuration de l'optimiseur avec gestion du weight decay\n    no_decay = ['bias', 'LayerNorm.weight']\n    parametres_optimiseur = [\n        {\n            'params': [p for n, p in modele.named_parameters() if not any(nd in n for nd in no_decay)],\n            'weight_decay': 0.01\n        },\n        {\n            'params': [p for n, p in modele.named_parameters() if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0\n        }\n    ]\n\n    # Initialisation de l'optimiseur et du plannificateur\n    nombre_etapes_totales = len(chargeur_donnees_entrainement) * nombre_epoques\n    optimiseur = AdamW(parametres_optimiseur, lr=taux_apprentissage)\n    plannificateur = get_linear_schedule_with_warmup(\n        optimiseur,\n        num_warmup_steps=0,\n        num_training_steps=nombre_etapes_totales\n    )\n\n    # Début de l'entraînement\n    logger.info(\"\\n========== Début de l'entraînement ==========\")\n    logger.info(f\"Nombre d'échantillons d'entraînement : {len(jeu_donnees_entrainement)}\")\n    logger.info(f\"Nombre d'époques : {nombre_epoques}\")\n    logger.info(f\"Taille des lots : {taille_lot}\")\n    \n    meilleures_metriques = {\n        'precision': 0,\n        'rappel': 0,\n        'f1': 0,\n        'epoque': 0\n    }\n\n    for epoque in range(nombre_epoques):\n        modele.train()\n        perte_totale = 0\n        predictions_totales = []\n        etiquettes_totales = []\n\n        with tqdm(chargeur_donnees_entrainement, \n                 desc=f\"Époque {epoque+1}/{nombre_epoques}\",\n                 unit=\"lot\") as iterateur_epoque:\n            \n            for etape, lots in enumerate(iterateur_epoque):\n                # Passe avant et calcul de la perte\n                perte, liste_logits = passe_avant_selection_gloses(modele, lots, appareil)\n                \n                # Collecte des prédictions\n                for logits_lot, lot in zip(liste_logits, lots):\n                    predictions = (logits_lot > 0.5).cpu().numpy().astype(int)\n                    etiquettes = lot[3].cpu().numpy()\n                    predictions_totales.extend(predictions)\n                    etiquettes_totales.extend(etiquettes)\n\n                # Rétropropagation et optimisation\n                optimiseur.zero_grad()\n                perte.backward()\n                torch.nn.utils.clip_grad_norm_(modele.parameters(), 1.0)\n                optimiseur.step()\n                plannificateur.step()\n\n                perte_totale += perte.item()\n                \n                # Calcul des métriques intermédiaires\n                precision_courante = precision_score(\n                    etiquettes_totales, \n                    predictions_totales, \n                    zero_division=0\n                )\n                \n                # Mise à jour de la barre de progression\n                iterateur_epoque.set_postfix({\n                    'Perte': f'{perte.item():.4f}',\n                    'Précision': f'{precision_courante:.4f}',\n                    'Taux Apprentissage': f'{plannificateur.get_last_lr()[0]:.6f}'\n                })\n\n        # Évaluation de fin d'époque\n        precision = precision_score(etiquettes_totales, predictions_totales, zero_division=0)\n        rappel = recall_score(etiquettes_totales, predictions_totales, zero_division=0)\n        f1 = f1_score(etiquettes_totales, predictions_totales, zero_division=0)\n\n        # Mise à jour des meilleures métriques\n        if f1 > meilleures_metriques['f1']:\n            meilleures_metriques.update({\n                'precision': precision,\n                'rappel': rappel,\n                'f1': f1,\n                'epoque': epoque + 1\n            })\n            \n            # Sauvegarde du meilleur modèle\n            os.makedirs(repertoire_sortie, exist_ok=True)\n            modele.save_pretrained(repertoire_sortie)\n            tokeniseur.save_pretrained(repertoire_sortie)\n\n        # Affichage des résultats de l'époque\n        logger.info(f\"\\nRésultats de l'époque {epoque+1}:\")\n        logger.info(f\"Précision: {precision:.4f}\")\n        logger.info(f\"Rappel: {rappel:.4f}\")\n        logger.info(f\"Score F1: {f1:.4f}\")\n\n    # Résumé final de l'entraînement\n    logger.info(\"\\n========== Entraînement terminé ==========\")\n    logger.info(f\"Meilleures métriques (époque {meilleures_metriques['epoque']}):\")\n    logger.info(f\"Précision: {meilleures_metriques['precision']:.4f}\")\n    logger.info(f\"Rappel: {meilleures_metriques['rappel']:.4f}\")\n    logger.info(f\"Score F1: {meilleures_metriques['f1']:.4f}\")\n    logger.info(f\"Modèle sauvegardé dans : {repertoire_sortie}\")\n\n    return meilleures_metriques","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:26:52.664613Z","iopub.execute_input":"2024-12-21T16:26:52.664937Z","iopub.status.idle":"2024-12-21T16:26:52.680921Z","shell.execute_reply.started":"2024-12-21T16:26:52.664909Z","shell.execute_reply":"2024-12-21T16:26:52.680056Z"},"id":"TyLGcxD3m87n","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def ecrire_predictions(repertoire_sortie, chemin_evaluation, predictions, suffixe=None):\n    \"\"\"\n    Écrit les prédictions dans un fichier de sortie CSV.\n\n    Le système sauvegarde les résultats du modèle avec un nommage flexible.\n    \"\"\"\n    os.makedirs(repertoire_sortie, exist_ok=True)\n\n    nom_base = os.path.splitext(os.path.basename(chemin_evaluation))[0]\n    nom_fichier_sortie = f\"{nom_base}_predictions\"\n    if suffixe:\n        nom_fichier_sortie += f\"_{suffixe}\"\n    nom_fichier_sortie += \".csv\"\n\n    chemin_complet_sortie = os.path.join(repertoire_sortie, nom_fichier_sortie)\n\n    try:\n        with open(chemin_evaluation, 'r', newline='', encoding='utf-8') as fichier_eval:\n            lecteur = csv.reader(fichier_eval)\n            donnees_originales = list(lecteur)\n\n        for i, prediction in enumerate(predictions):\n            if i < len(donnees_originales):\n                donnees_originales[i].append(str(prediction))\n\n        with open(chemin_complet_sortie, 'w', newline='', encoding='utf-8') as fichier_sortie:\n            ecrivain = csv.writer(fichier_sortie)\n            ecrivain.writerows(donnees_originales)\n\n        logger.info(f\"Prédictions écrites dans {chemin_complet_sortie}\")\n\n    except Exception as e:\n        logger.error(f\"Erreur lors de l'écriture des prédictions : {e}\")\n\ndef evaluer_wsd(\n    modele,\n    tokeniseur,\n    chemin_evaluation,\n    longueur_sequence_max=128,\n    taille_lot_evaluation=16,\n    repertoire_sortie='/kaggle/working/resultEval',\n    suffixe=None,\n    debut_plage_evaluation=None,\n    fin_plage_evaluation=None,\n    taille_max_evaluation=None\n):\n    \"\"\"\n    Évalue un modèle de désambiguïsation lexicale (WSD).\n\n    Le système calcule les performances détaillées sur un jeu de données de test.\n    \"\"\"\n    jeu_donnees_evaluation = charger_jeu_donnees(\n        chemin_evaluation,\n        tokeniseur,\n        longueur_sequence_max,\n        debut_plage_evaluation,\n        fin_plage_evaluation,\n        taille_max_evaluation\n    )\n\n    echantillonneur = SequentialSampler(jeu_donnees_evaluation)\n    chargeur_donnees_evaluation = DataLoader(\n        jeu_donnees_evaluation,\n        sampler=echantillonneur,\n        batch_size=taille_lot_evaluation,\n        collate_fn=regrouper_lots\n    )\n\n    logger.info(\"***** Début de l'évaluation *****\")\n    logger.info(f\"Nombre d'exemples : {len(jeu_donnees_evaluation)}\")\n    logger.info(f\"Taille des lots : {taille_lot_evaluation}\")\n\n    perte_evaluation = 0.0\n    nombre_etapes_evaluation = 0\n    predictions = []\n    verites_terrain = []\n    appareil = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    modele.to(appareil)\n    modele.eval()\n\n    for lots in tqdm(chargeur_donnees_evaluation, desc=\"Évaluation en cours\"):\n        with torch.no_grad():\n            perte, liste_logits = passe_avant_selection_gloses(modele, lots, appareil)\n\n        perte_evaluation += perte\n\n        # Collecte des prédictions et des vérités de terrain\n        for logits_lot, lot in zip(liste_logits, lots):\n            predictions.extend((logits_lot > 0.5).cpu().numpy().astype(int))\n            verites_terrain.extend(lot[3].cpu().numpy())\n\n        nombre_etapes_evaluation += 1\n\n    perte_evaluation = perte_evaluation / nombre_etapes_evaluation\n\n    # Calcul des métriques détaillées\n    accuracy = accuracy_score(verites_terrain, predictions)\n    balanced_accuracy = balanced_accuracy_score(verites_terrain, predictions)\n    precision = precision_score(verites_terrain, predictions, zero_division=0)\n    rappel = recall_score(verites_terrain, predictions, zero_division=0)\n    f1 = f1_score(verites_terrain, predictions, zero_division=0)\n    mcc = matthews_corrcoef(verites_terrain, predictions)\n    specificity = recall_score(verites_terrain, predictions, pos_label=0)\n    matrice_confusion = confusion_matrix(verites_terrain, predictions)\n    roc_auc = roc_auc_score(verites_terrain, predictions)\n\n    metriques = {\n        'perte': perte_evaluation,\n        'accuracy': accuracy,\n        'balanced_accuracy': balanced_accuracy,\n        'precision': precision,\n        'rappel': rappel,\n        'f1_score': f1,\n        'mcc': mcc,\n        'specificity': specificity,\n        'roc_auc': roc_auc,\n        'matrice_confusion': matrice_confusion\n    }\n\n    # Écriture des prédictions\n    ecrire_predictions(repertoire_sortie, chemin_evaluation, predictions, suffixe='prediction_v1')\n\n    # Journalisation des résultats détaillés\n    logger.info(\"\\n📊 Résultats d'évaluation :\")\n    for metrique, valeur in metriques.items():\n        if metrique != 'matrice_confusion':\n            logger.info(f\"{metrique.capitalize().replace('_', ' ')} : {valeur:.4f}\")\n    logger.info(\"\\nMatrice de confusion :\")\n    logger.info(np.array2string(metriques['matrice_confusion'], separator=', '))\n\n    # Création d'un rapport au format CSV\n    chemin_rapport = os.path.join(repertoire_sortie, f\"rapport_evaluation_{suffixe or 'defaut'}.csv\")\n    with open(chemin_rapport, 'w', newline='', encoding='utf-8') as fichier:\n        ecrivain = csv.writer(fichier)\n        ecrivain.writerow(['Métrique', 'Valeur'])\n        for metrique, valeur in metriques.items():\n            if metrique == 'matrice_confusion':\n                continue  # La matrice de confusion n'est pas incluse dans le CSV\n            ecrivain.writerow([metrique.capitalize().replace('_', ' '), f\"{valeur:.4f}\"])\n\n    logger.info(f\"Rapport d'évaluation sauvegardé dans {chemin_rapport}\")\n\n    return metriques\n\ndef creer_visualisations(metriques, repertoire_sortie, suffixe, verites_terrain):\n    \"\"\"Crée les visualisations des résultats.\"\"\"\n    try:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n\n        # Matrice de confusion\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(\n            metriques['matrice_confusion'],\n            annot=True,\n            fmt='d',\n            cmap='Blues',\n            xticklabels=['Classe 0', 'Classe 1'],\n            yticklabels=['Classe 0', 'Classe 1']\n        )\n        plt.xlabel(\"Prédictions\")\n        plt.ylabel(\"Vérité terrain\")\n        plt.title(\"Matrice de confusion\")\n        \n        chemin_matrice = os.path.join(repertoire_sortie, \n                                     f\"matrice_confusion_{suffixe or 'defaut'}.png\")\n        plt.savefig(chemin_matrice, bbox_inches='tight', dpi=300)\n        plt.close()\n\n        # Distribution des classes\n        plt.figure(figsize=(8, 6))\n        sns.countplot(x=verites_terrain)\n        plt.title(\"Distribution des classes\")\n        plt.xlabel(\"Classe\")\n        plt.ylabel(\"Nombre d'exemples\")\n        \n        chemin_distribution = os.path.join(repertoire_sortie,\n                                         f\"distribution_classes_{suffixe or 'defaut'}.png\")\n        plt.savefig(chemin_distribution, bbox_inches='tight', dpi=300)\n        plt.close()\n\n    except ImportError:\n        logger.warning(\"Seaborn ou Matplotlib non installé. Visualisations non générées.\")\n    except Exception as e:\n        logger.error(f\"Erreur lors de la création des visualisations : {e}\")\n\ndef obtenir_predictions(modele, tokeniseur, phrase):\n    \"\"\"\n    Obtient les prédictions de sens pour un mot ambigu dans une phrase.\n\n    Le système analyse les différents sens possibles et leurs probabilités.\n    \"\"\"\n    resultat = re.search(r\"\\[TGT\\](.*)\\[TGT\\]\", phrase)\n    if resultat is None:\n        print(\"\\nFormat d'entrée incorrect. Veuillez réessayer.\")\n        return\n\n    mot_ambigu = resultat.group(1).strip()\n    cles_sens = []\n    definitions = []\n    for cle_sens, definition in obtenir_glosses(mot_ambigu, None).items():\n        cles_sens.append(cle_sens)\n        definitions.append(definition)\n\n    LONGUEUR_SEQUENCE_MAX = 128\n    enregistrement = GlossSelectionRecord(\"test\", phrase, cles_sens, definitions, [-1])\n    caracteristiques = creer_caracteristiques_depuis_enregistrements([enregistrement], LONGUEUR_SEQUENCE_MAX, tokeniseur)[0]\n\n    appareil = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    modele.to(appareil)\n\n    with torch.no_grad():\n        logits = torch.zeros(len(definitions), dtype=torch.double).to(appareil)\n        for i, entree_bert in tqdm(list(enumerate(caracteristiques)), desc=\"Progression\"):\n            logits[i] = modele.couche_classement(\n                modele.bert(\n                    input_ids=torch.tensor(entree_bert.ids_entree, dtype=torch.long).unsqueeze(0).to(appareil),\n                    attention_mask=torch.tensor(entree_bert.masque_entree, dtype=torch.long).unsqueeze(0).to(appareil),\n                    token_type_ids=torch.tensor(entree_bert.ids_segment, dtype=torch.long).unsqueeze(0).to(appareil)\n                )[1]\n            )\n        scores = softmax(logits, dim=0)\n\n    return sorted(zip(cles_sens, definitions, scores), key=lambda x: x[-1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:27:07.321732Z","iopub.execute_input":"2024-12-21T16:27:07.322152Z","iopub.status.idle":"2024-12-21T16:27:07.341607Z","shell.execute_reply.started":"2024-12-21T16:27:07.322117Z","shell.execute_reply":"2024-12-21T16:27:07.340618Z"},"id":"anXM_SmPrFEr","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def fonction_principale():\n    \"\"\"\n    Fonction principale pour charger le modèle et lancer l'interaction utilisateur.\n\n    Le système permet de tester les prédictions de désambiguïsation lexicale.\n    \"\"\"\n    print(\"Chargement du modèle...\")\n    modele = BertWSD.from_pretrained(\"/kaggle/input/model-last\")\n    tokeniseur = BertTokenizer.from_pretrained(\"/kaggle/input/model-last\")\n    appareil = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    modele.to(appareil)\n    modele.eval()\n\n    while True:\n        phrase = input(\"\\nEntrez une phrase avec un mot ambigu entre balises [TGT]\\n> \")\n        predictions = obtenir_predictions(modele, tokeniseur, phrase)\n        if predictions:\n            print(\"\\nPrédictions:\")\n            print(tabulate(\n                [[f\"{i+1}.\", cle, gloss, f\"{score:.5f}\"] for i, (cle, gloss, score) in enumerate(predictions)],\n                headers=[\"N°\", \"Clé de sens\", \"Définition\", \"Score\"])\n            )","metadata":{"execution":{"iopub.status.busy":"2024-12-21T11:47:52.787439Z","iopub.execute_input":"2024-12-21T11:47:52.787726Z","iopub.status.idle":"2024-12-21T11:47:52.793064Z","shell.execute_reply.started":"2024-12-21T11:47:52.787701Z","shell.execute_reply":"2024-12-21T11:47:52.792085Z"},"id":"WcySBUMfrJnB","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Options d'exécution\n    #choix = input(\"Que voulez-vous faire ?\\n1. Tester des prédictions interactivement\\n2. Entraîner le modèle\\n3. Évaluer le modèle\\nVotre choix : \")\n\n    if choix == '1':\n        fonction_principale()\n    elif choix == '2':\n        resultats = entrainer_wsd(\n        chemin_entrainement= '/kaggle/input/datasett/corpus_dir-max_num_gloss5-augmented.csv',\n        chemin_evaluation= '/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv',\n        debut_plage_entrainement=50001,\n        fin_plage_entrainement= 263631,\n        taille_max_entrainement=150000,\n        debut_plage_evaluation=2001,\n        fin_plage_evaluation=38050,\n        taille_max_evaluation=22000\n        )\n    \n        print(f\"Meilleur score F1 obtenu : {resultats['f1']:.4f}\")\n        \n    elif choix == '3':\n        modele = BertWSD.from_pretrained('/kaggle/input/modell') #/kaggle/working/resultats\n        tokeniseur = BertTokenizer.from_pretrained('/kaggle/input/modell')\n        resultats = evaluer_wsd(\n            modele=modele,\n            tokeniseur=tokeniseur,\n            chemin_evaluation='/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv',\n            debut_plage_evaluation=None,\n            fin_plage_evaluation=None,\n            taille_max_evaluation=None\n    )\n\n        # Affichage des métriques si nécessaire\n        print(\"Précision:\", resultats['precision'])\n        print(\"Rappel:\", resultats['rappel'])\n        print(\"Score F1:\", resultats['f1_score'])\n        print(\"Matrice de Confusion:\\n\", resultats['matrice_confusion'])\n    else:\n        print(\"Choix invalide.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T11:01:43.474543Z","iopub.execute_input":"2024-12-20T11:01:43.474792Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Que voulez-vous faire ?\n1. Tester des prédictions interactivement\n2. Entraîner le modèle\n3. Évaluer le modèle\nVotre choix :  2\n"},{"name":"stderr","text":"Some weights of the model checkpoint at /kaggle/input/modell were not used when initializing BertWSD: ['ranking_linear.bias', 'ranking_linear.weight']\n- This IS expected if you are initializing BertWSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertWSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertWSD were not initialized from the model checkpoint at /kaggle/input/modell and are newly initialized: ['couche_classement.bias', 'couche_classement.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nConversion des données: 100%|██████████| 150000/150000 [02:51<00:00, 877.08it/s] \n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nÉpoque 1/3: 100%|██████████| 18750/18750 [4:52:12<00:00,  1.07lot/s, Perte=1.0915, Précision=0.8022, Taux Apprentissage=0.000033]  \nÉpoque 2/3: 100%|██████████| 18750/18750 [4:54:42<00:00,  1.06lot/s, Perte=0.3767, Précision=0.8303, Taux Apprentissage=0.000017]  \nÉpoque 3/3:  19%|█▊        | 3513/18750 [41:17<3:25:08,  1.24lot/s, Perte=0.2039, Précision=0.8496, Taux Apprentissage=0.000014]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Options d'exécution\n    choix = input(\"Que voulez-vous faire ?\\n1. Tester des prédictions interactivement\\n2. Entraîner le modèle\\n3. Évaluer le modèle\\nVotre choix : \")\n\n    if choix == '1':\n        fonction_principale()\n    elif choix == '2':\n        resultats = entrainer_wsd(\n        chemin_entrainement= '/kaggle/input/datasett/corpus_dir-max_num_gloss5-augmented.csv',\n        chemin_evaluation= '/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv',\n        debut_plage_entrainement=50001,\n        fin_plage_entrainement= 263631,\n        taille_max_entrainement=150000,\n        debut_plage_evaluation=2001,\n        fin_plage_evaluation=38050,\n        taille_max_evaluation=22000\n        )\n    \n        print(f\"Meilleur score F1 obtenu : {resultats['f1']:.4f}\")\n        \n    elif choix == '3':\n        \n        # Vérification des chemins des modèles et des données\n        chemin_modele = '/kaggle/input/modell'\n        chemin_donnees = '/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv'\n        \n        if not os.path.exists(chemin_modele):\n            print(f\"Erreur : Le chemin du modèle '{chemin_modele}' est introuvable.\")\n            exit(1)\n        if not os.path.exists(chemin_donnees):\n            print(f\"Erreur : Le fichier d'évaluation '{chemin_donnees}' est introuvable.\")\n            exit(1)\n        \n        # Chargement du modèle et du tokenizer\n        try:\n            modele = BertWSD.from_pretrained(chemin_modele)\n            tokeniseur = BertTokenizer.from_pretrained(chemin_modele)\n        except Exception as e:\n            print(f\"Erreur lors du chargement du modèle ou du tokenizer : {e}\")\n            exit(1)\n        \n        # Évaluation du modèle\n        try:\n            resultats = evaluer_wsd(\n                modele=modele,\n            tokeniseur=tokeniseur,\n            chemin_evaluation=chemin_donnees,\n            debut_plage_evaluation=2001,\n            fin_plage_evaluation=3001,\n            taille_max_evaluation=1000\n            \n            )\n        \n            # Affichage des résultats\n            print(\"\\n📊 Résultats d'évaluation :\")\n            print(\"Précision Globale (Accuracy):\", resultats['accuracy'])\n            print(f\"Précision : {resultats['precision']:.4f}\")\n            print(f\"Rappel : {resultats['rappel']:.4f}\")\n            print(f\"Score F1 : {resultats['f1_score']:.4f}\")\n            print(\"Score ROC-AUC:\", resultats['roc_auc'])\n            print(\"\\nMatrice de Confusion :\")\n            print(np.array2string(resultats['matrice_confusion'], separator=', '))\n\n        except Exception as e:\n            print(f\"Erreur lors de l'évaluation du modèle : {e}\")\n            exit(1)\n    else:\n        print(\"Choix invalide.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T00:28:22.210429Z","iopub.execute_input":"2024-12-21T00:28:22.210921Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Que voulez-vous faire ?\n1. Tester des prédictions interactivement\n2. Entraîner le modèle\n3. Évaluer le modèle\nVotre choix :  2\n"},{"name":"stderr","text":"Conversion des données: 100%|██████████| 150000/150000 [02:56<00:00, 849.49it/s] \n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nÉpoque 1/2: 100%|██████████| 18750/18750 [4:52:44<00:00,  1.07lot/s, Perte=0.0060, Précision=0.8509, Taux Apprentissage=0.000005]  \nÉpoque 2/2:  68%|██████▊   | 12716/18750 [3:00:36<1:47:42,  1.07s/lot, Perte=0.0328, Précision=0.8675, Taux Apprentissage=0.000002]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Options d'exécution\n    choix = input(\"Que voulez-vous faire ?\\n1. Tester des prédictions interactivement\\n2. Entraîner le modèle\\n3. Évaluer le modèle\\nVotre choix : \")\n\n    if choix == '1':\n        fonction_principale()\n    elif choix == '2':\n        resultats = entrainer_wsd(\n        chemin_entrainement= '/kaggle/input/datasett/corpus_dir-max_num_gloss5-augmented.csv',\n        chemin_evaluation= '/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv',\n        debut_plage_entrainement=50001,\n        fin_plage_entrainement= 263631,\n        taille_max_entrainement=150000,\n        debut_plage_evaluation=2001,\n        fin_plage_evaluation=38050,\n        taille_max_evaluation=22000\n        )\n    \n        print(f\"Meilleur score F1 obtenu : {resultats['f1']:.4f}\")\n        \n    elif choix == '3':\n        \n        # Vérification des chemins des modèles et des données\n        chemin_modele = '/kaggle/input/modell'\n        chemin_donnees = '/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv'\n        \n        if not os.path.exists(chemin_modele):\n            print(f\"Erreur : Le chemin du modèle '{chemin_modele}' est introuvable.\")\n            exit(1)\n        if not os.path.exists(chemin_donnees):\n            print(f\"Erreur : Le fichier d'évaluation '{chemin_donnees}' est introuvable.\")\n            exit(1)\n        \n        # Chargement du modèle et du tokenizer\n        try:\n            modele = BertWSD.from_pretrained(chemin_modele)\n            tokeniseur = BertTokenizer.from_pretrained(chemin_modele)\n        except Exception as e:\n            print(f\"Erreur lors du chargement du modèle ou du tokenizer : {e}\")\n            exit(1)\n        \n        # Évaluation du modèle\n        try:\n            resultats = evaluer_wsd(\n                modele=modele,\n            tokeniseur=tokeniseur,\n            chemin_evaluation=chemin_donnees,\n            debut_plage_evaluation=2001,\n            fin_plage_evaluation=3001,\n            taille_max_evaluation=1000\n            \n            )\n        \n            # Affichage des résultats\n            print(\"\\n📊 Résultats d'évaluation :\")\n            print(\"Précision Globale (Accuracy):\", resultats['accuracy'])\n            print(f\"Précision : {resultats['precision']:.4f}\")\n            print(f\"Rappel : {resultats['rappel']:.4f}\")\n            print(f\"Score F1 : {resultats['f1_score']:.4f}\")\n            print(\"Score ROC-AUC:\", resultats['roc_auc'])\n            print(\"\\nMatrice de Confusion :\")\n            print(np.array2string(resultats['matrice_confusion'], separator=', '))\n\n        except Exception as e:\n            print(f\"Erreur lors de l'évaluation du modèle : {e}\")\n            exit(1)\n    else:\n        print(\"Choix invalide.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:49:00.879983Z","iopub.execute_input":"2024-12-21T11:49:00.880306Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Que voulez-vous faire ?\n1. Tester des prédictions interactivement\n2. Entraîner le modèle\n3. Évaluer le modèle\nVotre choix :  2\n"},{"name":"stderr","text":"Conversion des données: 100%|██████████| 150000/150000 [02:56<00:00, 850.74it/s] \n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nÉpoque 1/1:  77%|███████▋  | 14348/18750 [3:22:25<1:18:12,  1.07s/lot, Perte=0.0000, Précision=0.9124, Taux Apprentissage=0.000002]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n\n  # Options d'exécution\n  choix = input(\"Que voulez-vous faire ?\\n1. Tester des prédictions interactivement\\n2. Entraîner le modèle\\n3. Évaluer le modèle\\nVotre choix : \")\n\n  if choix == '1':\n      fonction_principale()\n  elif choix == '2':\n\n    resultats = entrainer_wsd(\n    chemin_entrainement= '/kaggle/input/dataset/corpus_dir-max_num_gloss5-augmented.csv',\n    chemin_evaluation= '/kaggle/input/dataset/semeval2007-max_num_gloss5-augmented.csv',\n    debut_plage_entrainement=50001,\n    fin_plage_entrainement= 263631,\n    taille_max_entrainement=150000,\n    debut_plage_evaluation=2001,\n    fin_plage_evaluation=38050,\n    taille_max_evaluation=22000\n    )\n\n    print(f\"Meilleur score F1 obtenu : {resultats['f1']:.4f}\")\n\n  elif choix == '3':\n      modele = BertWSD.from_pretrained('/kaggle/input/model-last')\n      tokeniseur = BertTokenizer.from_pretrained('/kaggle/input/model-last')\n      resultats = evaluer_wsd(\n          modele=modele,\n          tokeniseur=tokeniseur,\n          chemin_evaluation='/kaggle/input/datasett/semeval2007-max_num_gloss5-augmented.csv',\n          debut_plage_evaluation=2001,\n          fin_plage_evaluation=12001, #38050,\n          taille_max_evaluation=10000\n      )\n\n      print(\"\\n📊 Résultats de l'évaluation :\")\n\n      for metrique, valeur in resultats.items():\n          if metrique == 'matrice_confusion':\n              print(f\"{metrique.capitalize()} :\\n{valeur}\")\n          else:\n              print(f\"{metrique.capitalize()} : {valeur:.4f}\")\n  else:\n    print(\"Choix invalide.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T16:30:56.146992Z","iopub.execute_input":"2024-12-21T16:30:56.147378Z","iopub.status.idle":"2024-12-21T16:35:33.212689Z","shell.execute_reply.started":"2024-12-21T16:30:56.147348Z","shell.execute_reply":"2024-12-21T16:35:33.211790Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Que voulez-vous faire ?\n1. Tester des prédictions interactivement\n2. Entraîner le modèle\n3. Évaluer le modèle\nVotre choix :  3\n"},{"name":"stderr","text":"Conversion des données: 100%|██████████| 10000/10000 [00:08<00:00, 1236.63it/s]\nÉvaluation en cours: 100%|██████████| 625/625 [04:21<00:00,  2.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Résultats de l'évaluation :\nPerte : 2.8822\nAccuracy : 0.7390\nBalanced_accuracy : 0.6536\nPrecision : 0.5454\nRappel : 0.4572\nF1_score : 0.4974\nMcc : 0.3253\nSpecificity : 0.8500\nRoc_auc : 0.6536\nMatrice_confusion :\n[[21588  3811]\n [ 5428  4572]]\n","output_type":"stream"}],"execution_count":9}]}