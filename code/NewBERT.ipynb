{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Bienvenue dans Colaboratory","provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10208449,"sourceType":"datasetVersion","datasetId":6309077},{"sourceId":10217016,"sourceType":"datasetVersion","datasetId":6315470}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\n\n# Télécharger WordNet dans un répertoire spécifique\nnltk.download('wordnet', download_dir='/kaggle/working//nltk_data')\n\n# Ajouter le chemin des données téléchargées\nnltk.data.path.append('/kaggle/working//nltk_data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:02:29.439864Z","iopub.execute_input":"2024-12-16T14:02:29.440702Z","iopub.status.idle":"2024-12-16T14:02:30.838651Z","shell.execute_reply.started":"2024-12-16T14:02:29.440651Z","shell.execute_reply":"2024-12-16T14:02:30.837552Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working//nltk_data...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Chemin du fichier ZIP\nzip_path = \"/kaggle/working/nltk_data/corpora/wordnet.zip\"\nextract_path = \"/kaggle/working/nltk_data/corpora/\"  # Chemin où extraire les fichiers\n\n# Décompression\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_path)\n\nprint(f\"Contenu extrait dans {extract_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:04:32.920750Z","iopub.execute_input":"2024-12-16T14:04:32.921195Z","iopub.status.idle":"2024-12-16T14:04:33.194304Z","shell.execute_reply.started":"2024-12-16T14:04:32.921157Z","shell.execute_reply":"2024-12-16T14:04:33.193055Z"}},"outputs":[{"name":"stdout","text":"Contenu extrait dans /kaggle/working/nltk_data/corpora/\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Tester si WordNet fonctionne\nfrom nltk.corpus import wordnet as wn\nprint(wn.synsets('bank'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:04:55.071629Z","iopub.execute_input":"2024-12-16T14:04:55.072015Z","iopub.status.idle":"2024-12-16T14:04:56.560296Z","shell.execute_reply.started":"2024-12-16T14:04:55.071983Z","shell.execute_reply":"2024-12-16T14:04:56.559221Z"}},"outputs":[{"name":"stdout","text":"[Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport csv\nimport math\nimport random\nimport logging\nimport argparse\nimport itertools\nfrom tabulate import tabulate\nfrom collections import namedtuple\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.metrics import precision_score\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import (\n    BertModel, BertConfig, BertPreTrainedModel,\n    BertTokenizer, AdamW, get_linear_schedule_with_warmup\n)","metadata":{"id":"pesTpjUR0fqj","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T15:03:18.950719Z","iopub.execute_input":"2024-12-16T15:03:18.951156Z","iopub.status.idle":"2024-12-16T15:03:18.978689Z","shell.execute_reply.started":"2024-12-16T15:03:18.951120Z","shell.execute_reply":"2024-12-16T15:03:18.977419Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Configuration des journaux\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Structures de données\nGlossSelectionRecord = namedtuple(\"GlossSelectionRecord\", [\"guid\", \"sentence\", \"sense_keys\", \"glosses\", \"targets\"])\nBertInput = namedtuple(\"BertInput\", [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_id\"])","metadata":{"id":"_yaIyehX1MSp","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:20:20.547252Z","iopub.execute_input":"2024-12-16T14:20:20.547943Z","iopub.status.idle":"2024-12-16T14:20:20.553824Z","shell.execute_reply.started":"2024-12-16T14:20:20.547905Z","shell.execute_reply":"2024-12-16T14:20:20.552770Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class WSDDataset(Dataset):\n    def __init__(self, features):\n        self.features = features\n\n    def __getitem__(self, index):\n        return self.features[index]\n\n    def __len__(self):\n        return len(self.features)","metadata":{"id":"ChU6eVKR1ShA","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:20:24.178029Z","iopub.execute_input":"2024-12-16T14:20:24.178819Z","iopub.status.idle":"2024-12-16T14:20:24.184126Z","shell.execute_reply.started":"2024-12-16T14:20:24.178781Z","shell.execute_reply":"2024-12-16T14:20:24.183113Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class BertWSD(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n        self.ranking_linear = torch.nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n\ndef _compute_weighted_loss(loss, weighting_factor):\n    \"\"\"Calcul d'une perte pondérée\"\"\"\n    squared_factor = weighting_factor ** 2\n    return 1 / (2 * squared_factor) * loss + math.log(1 + squared_factor)\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Tronque une paire de séquences à la longueur maximale\"\"\"\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","metadata":{"id":"m5SyyrL91MYW","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:20:27.612736Z","iopub.execute_input":"2024-12-16T14:20:27.613083Z","iopub.status.idle":"2024-12-16T14:20:27.620792Z","shell.execute_reply.started":"2024-12-16T14:20:27.613054Z","shell.execute_reply":"2024-12-16T14:20:27.619597Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def load_dataset(\n    csv_path, \n    tokenizer, \n    max_sequence_length, \n    max_samples=None\n):\n    \"\"\"\n    Charge le jeu de données à partir d'un fichier CSV avec option de sous-échantillonnage.\n    \n    Args:\n        csv_path (str): Chemin vers le fichier CSV\n        tokenizer (BertTokenizer): Tokenizer BERT\n        max_sequence_length (int): Longueur maximale des séquences\n        max_samples (int, optional): Nombre maximal d'échantillons à charger\n    \n    Returns:\n        WSDDataset: Jeu de données pour l'entraînement ou l'évaluation\n    \"\"\"\n    def _deserialize_csv_record(row):\n        return GlossSelectionRecord(\n            row[0],  # guid\n            row[1],  # sentence\n            eval(row[2]),  # sense_keys\n            eval(row[3]),  # glosses\n            [int(t) for t in eval(row[4])]  # targets\n        )\n\n    def _create_records_from_csv(csv_path, deserialize_fn, max_samples=None):\n        \"\"\"\n        Crée des enregistrements à partir d'un fichier CSV avec sous-échantillonnage.\n        \n        Args:\n            csv_path (str): Chemin du fichier CSV\n            deserialize_fn (callable): Fonction de désérialisation\n            max_samples (int, optional): Nombre maximal d'échantillons\n        \n        Returns:\n            list: Liste d'enregistrements\n        \"\"\"\n        records = []\n        with open(csv_path, 'r', encoding='utf-8', newline='') as f:\n            reader = csv.reader(f)\n            next(reader)  # Ignorer l'en-tête\n            \n            # Utiliser itertools pour limiter les échantillons\n            for row in itertools.islice(reader, max_samples):\n                records.append(deserialize_fn(row))\n        \n        return records\n\n    # Charger les enregistrements avec limitation optionnelle\n    records = _create_records_from_csv(\n        csv_path, \n        _deserialize_csv_record, \n        max_samples\n    )\n    \n    # Convertir en features\n    features = _create_features_from_records(\n        records, \n        max_sequence_length, \n        tokenizer\n    )\n    \n    # Log du nombre d'échantillons chargés\n    logger.info(f\"Chargé {len(features)} échantillons depuis {csv_path}\")\n    \n    return WSDDataset(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:20:32.935290Z","iopub.execute_input":"2024-12-16T14:20:32.936163Z","iopub.status.idle":"2024-12-16T14:20:32.944876Z","shell.execute_reply.started":"2024-12-16T14:20:32.936125Z","shell.execute_reply":"2024-12-16T14:20:32.943884Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def _create_features_from_records(records, max_seq_length, tokenizer):\n    \"\"\"Convertit les enregistrements en features pour BERT\"\"\"\n    features = []\n    for record in tqdm(records, desc=\"Conversion des données\"):\n        tokens_a = tokenizer.tokenize(record.sentence)\n        sequences = [(gloss, 1 if i in record.targets else 0) for i, gloss in enumerate(record.glosses)]\n\n        pairs = []\n        for seq, label in sequences:\n            tokens_b = tokenizer.tokenize(seq)\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n\n            tokens = tokens_a + ['[SEP]']\n            segment_ids = [0] * len(tokens)\n\n            tokens += tokens_b + ['[SEP]']\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n            tokens = ['[CLS]'] + tokens\n            segment_ids = [0] + segment_ids\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n            input_mask = [1] * len(input_ids)\n\n            padding_length = max_seq_length - len(input_ids)\n            input_ids += [0] * padding_length\n            input_mask += [0] * padding_length\n            segment_ids += [0] * padding_length\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            pairs.append(\n                BertInput(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label)\n            )\n\n        features.append(pairs)\n\n    return features\n\ndef collate_batch(batch):\n    \"\"\"Regroupe les lots de données\"\"\"\n    max_seq_length = len(batch[0][0].input_ids)\n\n    collated = []\n    for sub_batch in batch:\n        batch_size = len(sub_batch)\n        sub_collated = [torch.zeros([batch_size, max_seq_length], dtype=torch.long) for _ in range(3)] + \\\n                       [torch.zeros([batch_size], dtype=torch.long)]\n\n        for i, bert_input in enumerate(sub_batch):\n            sub_collated[0][i] = torch.tensor(bert_input.input_ids, dtype=torch.long)\n            sub_collated[1][i] = torch.tensor(bert_input.input_mask, dtype=torch.long)\n            sub_collated[2][i] = torch.tensor(bert_input.segment_ids, dtype=torch.long)\n            sub_collated[3][i] = torch.tensor(bert_input.label_id, dtype=torch.long)\n\n        collated.append(sub_collated)\n\n    return collated\n\ndef forward_gloss_selection(model, batches, device):\n    \"\"\"Effectue une passe avant pour la sélection de gloses\"\"\"\n    batch_loss = 0\n    logits_list = []\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    for batch in batches:\n        batch = tuple(t.to(device) for t in batch)\n        outputs = model.bert(input_ids=batch[0], attention_mask=batch[1], token_type_ids=batch[2])\n        hidden_state = model.dropout(outputs[1])\n\n        logits = model.ranking_linear(hidden_state).squeeze(-1)\n        labels = torch.max(batch[3], -1).indices.detach()\n        batch_loss += loss_fn(logits.unsqueeze(dim=0), labels.unsqueeze(dim=-1))\n        logits_list.append(logits)\n\n    loss = batch_loss / len(batches)\n    return loss, logits_list","metadata":{"id":"ZEKmdd0J1Sj6","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:20:48.506211Z","iopub.execute_input":"2024-12-16T14:20:48.506679Z","iopub.status.idle":"2024-12-16T14:20:48.522031Z","shell.execute_reply.started":"2024-12-16T14:20:48.506643Z","shell.execute_reply":"2024-12-16T14:20:48.520873Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def train_wsd(train_path, eval_path, output_dir='./results',max_train_samples=None, max_eval_samples=None):\n    \"\"\"\n    Entraînement du modèle de sélection de gloses avec sous-échantillonnage.\n    \n    Args:\n        train_path (str): Chemin du fichier CSV d'entraînement\n        eval_path (str): Chemin du fichier CSV d'évaluation\n        output_dir (str, optional): Répertoire de sauvegarde du modèle\n        max_train_samples (int, optional): Nombre maximal d'échantillons d'entraînement\n        max_eval_samples (int, optional): Nombre maximal d'échantillons d'évaluation\n    \"\"\"\n    # Configuration\n    max_seq_length = 128\n    batch_size = 8\n    num_train_epochs = 3\n    learning_rate = 5e-5\n    seed = 42\n\n    # Configuration du seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # Périphérique\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(f\"Utilisation du périphérique : {device}\")\n\n    # Modèle et Tokenizer\n    model_name = 'bert-base-cased'\n    config = BertConfig.from_pretrained(model_name, num_labels=2)\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    model = BertWSD.from_pretrained(model_name, config=config)\n\n    # Ajout du token spécial\n    if '[TGT]' not in tokenizer.additional_special_tokens:\n        tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n        model.resize_token_embeddings(len(tokenizer))\n\n    model.to(device)\n    \n    # Chargement des données avec sous-échantillonnage optionnel\n    train_dataset = load_dataset(\n        train_path, \n        tokenizer, \n        max_sequence_length=max_seq_length, \n        max_samples=max_train_samples\n    )\n\n    train_sampler = RandomSampler(train_dataset)\n    \n    train_dataloader = DataLoader(\n        train_dataset, \n        sampler=train_sampler, \n        batch_size=batch_size, \n        collate_fn=collate_batch\n    )\n    \n    # Optionnel : charger le jeu de données d'évaluation\n    if max_eval_samples is not None:\n        eval_dataset = load_dataset(\n            eval_path, \n            tokenizer, \n            max_sequence_length=max_seq_length, \n            max_samples=max_eval_samples\n        )\n    \n    # Préparation de l'optimiseur\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n            'weight_decay': 0.01\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n            'weight_decay': 0.0\n        }\n    ]\n\n    total_steps = len(train_dataloader) * num_train_epochs\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=total_steps\n    )\n    \n    logger.info(\"🚀 Entrainement Model Word Sense Disambiguation \")\n    logger.info(f\"Nombre total de lots de formatio (Batches): {len(train_dataloader)}\")\n    logger.info(f\"Device: {device}\")\n    \n    # Boucle d'entraînement\n    for epoch in range(num_train_epochs):\n        model.train()\n        total_loss = 0\n\n        # Track predictions and labels for precision calculation\n        all_preds = []\n        all_labels = []\n\n        # Create epoch progress bar\n        with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_train_epochs}\", \n                          unit=\"batch\", colour=\"green\") as epoch_iterator:\n        \n            start_time = time.time()\n    \n            for step, batches in enumerate(epoch_iterator):\n                loss, logits_list = forward_gloss_selection(model, batches, device)\n\n                # Collect predictions and true labels\n                for batch_logits, batch in zip(logits_list, batches):\n                    # Convert logits to predictions\n                    preds = (batch_logits > 0.5).cpu().numpy().astype(int)\n                    \n                    # Get true labels\n                    labels = batch[3].cpu().numpy()\n                    \n                    all_preds.extend(preds)\n                    all_labels.extend(labels)\n    \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n                optimizer.step()\n                scheduler.step()\n    \n                total_loss += loss.item()\n\n                # Calculate precision\n                try:\n                    precision = precision_score(all_labels, all_preds, zero_division=0)\n                except:\n                    precision = 0\n    \n                 # Update progress bar with real-time metrics\n                epoch_iterator.set_postfix({\n                    'Loss': f'{loss.item():.4f}', \n                    'Avg Loss': f'{total_loss/(step+1):.4f}',\n                    'Precision': f'{precision:.4f}',\n                    'Learning Rate': f'{scheduler.get_last_lr()[0]:.6f}'\n                })\n    \n                # End of epoch summary\n                epoch_duration = time.time() - start_time\n                # End of epoch summary\n                final_precision = precision_score(all_labels, all_preds, zero_division=0)\n                logger.info(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds. \"\n                            f\"Average Loss: {total_loss/len(train_dataloader):.4f}\"\n                            f\"Precision: {final_precision:.4f}\")\n    \n    \n                if step % 100 == 0:\n                    logger.info(f\"Époque {epoch}, Étape {step}, Perte : {loss.item()}\")\n\n    logger.info(\"✅ Entrainement terminé avec succès\")\n    \"\"\"   # Sauvegarde du modèle\n    os.makedirs(output_dir, exist_ok=True)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\"\"\"\n\n    # Sauvegarder tout le modèle\n    os.makedirs(output_dir, exist_ok=True)\n    torch.save(model, '/kaggle/working/modelPytorch/on_modele.pth')\n    \n    logger.info(f\"Modèle entraîné et sauvegardé dans {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:22:24.781657Z","iopub.execute_input":"2024-12-16T14:22:24.782038Z","iopub.status.idle":"2024-12-16T14:22:24.799731Z","shell.execute_reply.started":"2024-12-16T14:22:24.782007Z","shell.execute_reply":"2024-12-16T14:22:24.798588Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Charger seulement 5000 échantillons pour un test rapide\ntrain_wsd(\n    train_path='/kaggle/input/dataset/corpus_dir-max_num_gloss5-augmented.csv', \n    eval_path='/kaggle/input/dataset/semeval2007-max_num_gloss5-augmented.csv', \n    max_train_samples=50000, #40000==4heure      # Limiter à 5000 échantillons\n    max_eval_samples=10000  #8000==20%      # Limiter à 1000 échantillons d'évaluation\n)\n\n# Charger tous les échantillons (comportement par défaut)\n# train_wsd(train_path='/chemin/vers/train.csv', eval_path='/chemin/vers/eval.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:57:29.212395Z","iopub.execute_input":"2024-12-16T04:57:29.213092Z","iopub.status.idle":"2024-12-16T09:22:13.973628Z","shell.execute_reply.started":"2024-12-16T04:57:29.213056Z","shell.execute_reply":"2024-12-16T09:22:13.972671Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c2e144947f49bfbb3a058bf6460e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7445dbd7204983a848a2178a88beed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75b6eb1432b430f84c5aa378ac55035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2dba7f80b54b508a0d362fe022c6fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4b64a34faa434aa44674ea0acea390"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertWSD were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['ranking_linear.bias', 'ranking_linear.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/50000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6fedd24c094f6c881f3fb751de4255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"537bb4e62af74a90b1c9b72556ef7bf4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/6250 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ec1f1e16a74d78b83a11dd66f1fd6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/6250 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1a8930d8f3c46faa1d7e06a6cfca36c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/6250 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55638ac075fe4d248f3bae932978c7bd"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import os\nimport csv\n\ndef write_predictions(output_dir, eval_path, predictions, suffix=None):\n    \"\"\"\n    Écrit les prédictions dans un fichier de sortie.\n    \n    Args:\n        output_dir (str): Répertoire de sortie pour les fichiers de prédictions\n        eval_path (str): Chemin du fichier d'évaluation original\n        predictions (list): Liste des prédictions du modèle\n        suffix (str, optional): Suffixe à ajouter au nom du fichier de sortie\n    \"\"\"\n    # Créer le répertoire de sortie s'il n'existe pas\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Générer le nom de fichier de sortie\n    base_filename = os.path.splitext(os.path.basename(eval_path))[0]\n    output_filename = f\"{base_filename}_predictions\"\n    if suffix:\n        output_filename += f\"_{suffix}\"\n    output_filename += \".csv\"\n    \n    # Chemin complet du fichier de sortie\n    output_path = os.path.join(output_dir, output_filename)\n    \n    # Lire le fichier d'évaluation original\n    try:\n        with open(eval_path, 'r', newline='', encoding='utf-8') as eval_file:\n            reader = csv.reader(eval_file)\n            original_data = list(reader)\n        \n        # Ajouter les prédictions aux données originales\n        for i, pred in enumerate(predictions):\n            if i < len(original_data):\n                original_data[i].append(str(pred))\n        \n        # Écrire les données avec prédictions\n        with open(output_path, 'w', newline='', encoding='utf-8') as output_file:\n            writer = csv.writer(output_file)\n            writer.writerows(original_data)\n        \n        logger.info(f\"Prédictions écrites dans {output_path}\")\n    \n    except Exception as e:\n        logger.error(f\"Erreur lors de l'écriture des prédictions : {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:22:47.739170Z","iopub.execute_input":"2024-12-16T14:22:47.740216Z","iopub.status.idle":"2024-12-16T14:22:47.748574Z","shell.execute_reply.started":"2024-12-16T14:22:47.740175Z","shell.execute_reply":"2024-12-16T14:22:47.747446Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def evaluate(\n    model, \n    tokenizer, \n    eval_path, \n    max_seq_length=128, \n    eval_batch_size=16, \n    output_dir='./results',\n    suffix=None,\n    max_eval_samples=None  # Nouveau paramètre pour limiter le nombre d'exemples\n):\n    \"\"\"\n    Évalue un modèle de désambiguïsation lexicale (WSD).\n    \n    Args:\n        model: Modèle de WSD à évaluer\n        tokenizer: Tokenizer correspondant au modèle\n        eval_path: Chemin vers le jeu de données d'évaluation\n        max_seq_length: Longueur maximale des séquences\n        eval_batch_size: Taille des lots pour l'évaluation\n        output_dir: Répertoire de sortie pour les prédictions\n        suffix: Suffixe optionnel pour les fichiers de sortie\n        max_eval_samples: Nombre maximal d'exemples à évaluer (optionnel)\n    \n    Returns:\n        float: Perte moyenne d'évaluation\n    \"\"\"\n    # Charger le dataset d'évaluation\n    eval_dataset = load_dataset(\n        eval_path, \n        tokenizer, \n        max_sequence_length=max_seq_length,\n        max_samples=max_eval_samples\n    )\n    \n    # Limiter le nombre d'exemples si max_eval_samples est spécifié\n    if max_eval_samples is not None:\n        eval_dataset = eval_dataset[:max_eval_samples]\n        logger.info(f\"Nombre d'exemples limité à : {len(eval_dataset)}\")\n    \n    # Créer le DataLoader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset,\n        sampler=eval_sampler, \n        batch_size=eval_batch_size,\n        collate_fn=collate_batch\n    )\n    \n    # Préparer l'évaluation\n    logger.info(\"***** Début de l'évaluation *****\")\n    logger.info(f\"Nombre d'exemples : {len(eval_dataset)}\")\n    logger.info(f\"Taille des lots : {eval_batch_size}\")\n    \n    # Variables de suivi\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    predictions = []\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    # Mode évaluation\n    model.eval()\n    \n    # Évaluation\n    for batches in tqdm(eval_dataloader, desc=\"Évaluation en cours\"):\n        with torch.no_grad():\n            # Utiliser votre fonction de perte spécifique\n            loss, logits_list = forward_gloss_selection(model, batches, device)\n        \n        # Collecter les pertes et prédictions\n        eval_loss += loss\n        predictions.extend([torch.argmax(logits, dim=-1).item() for logits in logits_list])\n        nb_eval_steps += 1\n    \n    # Calculer la perte moyenne\n    eval_loss = eval_loss / nb_eval_steps\n    \n    # Écrire les prédictions\n    write_predictions(output_dir, eval_path, predictions, suffix='prediction_v1')\n    \n    return eval_loss.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:31:41.375773Z","iopub.execute_input":"2024-12-16T14:31:41.376194Z","iopub.status.idle":"2024-12-16T14:31:41.386812Z","shell.execute_reply.started":"2024-12-16T14:31:41.376156Z","shell.execute_reply":"2024-12-16T14:31:41.385722Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Exemple d'utilisation\ndef main():\n    # Charger votre modèle et tokenizer\n    model = BertWSD.from_pretrained('/kaggle/working/mesModels')\n    tokenizer = BertTokenizer.from_pretrained('/kaggle/working/mesModels')\n    \n    # Chemin vers votre jeu de données d'évaluation\n    eval_path = '/kaggle/input/dataset/semeval2007-max_num_gloss5-augmented.csv'\n    \n    # Évaluer le modèle\n    loss = evaluate(\n        model=model, \n        tokenizer=tokenizer, \n        eval_path=eval_path,\n        max_seq_length=128,\n        eval_batch_size=16,\n        output_dir='./evaluation_results',\n        max_eval_samples=1000\n    )\n\n    print(f\"Perte d'évaluation : {loss}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:32:26.474915Z","iopub.execute_input":"2024-12-16T14:32:26.475261Z","iopub.status.idle":"2024-12-16T14:46:06.812192Z","shell.execute_reply.started":"2024-12-16T14:32:26.475232Z","shell.execute_reply":"2024-12-16T14:46:06.810910Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47557debda1d42eea084341038a465d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Évaluation en cours:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c234345a7f4042669ade4a838e754b6a"}},"metadata":{}},{"name":"stdout","text":"Perte d'évaluation : 2.259589433670044\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import re\nimport torch\nfrom torch.nn.functional import softmax\nfrom tqdm import tqdm\n\n# S'assurer que get_glosses est importée ou définie\nfrom nltk.corpus import wordnet as wn\n\nWORDNET_POS = {'VERB': wn.VERB, 'NOUN': wn.NOUN, 'ADJ': wn.ADJ, 'ADV': wn.ADV}\n\ndef get_glosses(lemma, pos):\n    \"\"\"\n    Récupère les définitions (glosses) pour un mot donné.\n    \"\"\"\n    results = dict()\n    wn_pos = WORDNET_POS.get(pos, None) if pos is not None else None\n    morphemes = wn._morphy(lemma, pos=wn_pos) if pos is not None else []\n    for synset in set(wn.synsets(lemma, pos=wn_pos)):\n        sense_key = None\n        for l in synset.lemmas():\n            if l.name().lower() == lemma.lower():\n                sense_key = l.key()\n                break\n            elif l.name().lower() in morphemes:\n                sense_key = l.key()\n        if sense_key is not None:\n            results[sense_key] = synset.definition()\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:48:31.680268Z","iopub.execute_input":"2024-12-16T14:48:31.680764Z","iopub.status.idle":"2024-12-16T14:48:31.690796Z","shell.execute_reply.started":"2024-12-16T14:48:31.680727Z","shell.execute_reply":"2024-12-16T14:48:31.689134Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def get_predictions(model, tokenizer, sentence):\n    re_result = re.search(r\"\\[TGT\\](.*)\\[TGT\\]\", sentence)\n    if re_result is None:\n        print(\"\\nIncorrect input format. Please try again.\")\n        return\n\n    ambiguous_word = re_result.group(1).strip()\n    sense_keys = []\n    definitions = []\n    for sense_key, definition in get_glosses(ambiguous_word, None).items():\n        sense_keys.append(sense_key)\n        definitions.append(definition)\n\n    MAX_SEQ_LENGTH = 128\n    record = GlossSelectionRecord(\"test\", sentence, sense_keys, definitions, [-1])\n    features = _create_features_from_records([record], MAX_SEQ_LENGTH, tokenizer,\n                                             )[0]\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    with torch.no_grad():\n        logits = torch.zeros(len(definitions), dtype=torch.double).to(device)\n        for i, bert_input in tqdm(list(enumerate(features)), desc=\"Progress\"):\n            logits[i] = model.ranking_linear(\n                model.bert(\n                    input_ids=torch.tensor(bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(device),\n                    attention_mask=torch.tensor(bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(device),\n                    token_type_ids=torch.tensor(bert_input.segment_ids, dtype=torch.long).unsqueeze(0).to(device)\n                )[1]\n            )\n        scores = softmax(logits, dim=0)\n\n    return sorted(zip(sense_keys, definitions, scores), key=lambda x: x[-1], reverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T15:01:51.084472Z","iopub.execute_input":"2024-12-16T15:01:51.085303Z","iopub.status.idle":"2024-12-16T15:01:51.095257Z","shell.execute_reply.started":"2024-12-16T15:01:51.085256Z","shell.execute_reply":"2024-12-16T15:01:51.094206Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"cls_token=tokenizer.cls_token,\n                                             sep_token=tokenizer.sep_token,\n                                             cls_token_segment_id=1,\n                                             pad_token_segment_id=0,\n                                             disable_progress_bar=True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Load fine-tuned model and vocabulary\n    print(\"Loading model...\")\n    model = BertWSD.from_pretrained(\"/kaggle/working/mesModels\")\n    tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/mesModels\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    while True:\n        sentence = input(\"\\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\\n> \")\n        predictions = get_predictions(model, tokenizer, sentence)\n        if predictions:\n            print(\"\\nPredictions:\")\n            print(tabulate(\n                [[f\"{i+1}.\", key, gloss, f\"{score:.5f}\"] for i, (key, gloss, score) in enumerate(predictions)],\n                headers=[\"No.\", \"Sense key\", \"Definition\", \"Score\"])\n            )\n            # for i, (sense_key, definition, score) in enumerate(predictions):\n            #     # print(f\"  {i + 1:>3}. sense key: {sense_key:<15} score: {score:<8.5f} definition: {definition}\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T15:03:32.209715Z","iopub.execute_input":"2024-12-16T15:03:32.210181Z","iopub.status.idle":"2024-12-16T15:23:40.490181Z","shell.execute_reply.started":"2024-12-16T15:03:32.210145Z","shell.execute_reply":"2024-12-16T15:23:40.488654Z"}},"outputs":[{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  \n"},{"name":"stdout","text":"\nIncorrect input format. Please try again.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  He caught a [TGT] bass [TGT] yesterday.\n"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206b629092204de5a86173c95c123896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Progress:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d9cf35904c49f5a2094da8f2055ae5"}},"metadata":{}},{"name":"stdout","text":"\nPredictions:\n  No.  Sense key            Definition                                                                                             Score\n-----  -------------------  ---------------------------------------------------------------------------------------------------  -------\n    1  bass%1:06:02::       the member with the lowest range of a family of musical instruments                                  0.50121\n    2  bass%1:05:00::       nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes               0.44852\n    3  bass%1:13:01::       any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)  0.01779\n    4  bass%1:07:01::       the lowest part of the musical range                                                                 0.01545\n    5  bass%1:13:02::       the lean flesh of a saltwater fish of the family Serranidae                                          0.00991\n    6  bass%1:10:01::       the lowest part in polyphonic music                                                                  0.00201\n    7  bass%1:18:00::       an adult male singer with the lowest voice                                                           0.00179\n    8  bass%5:00:00:low:03  having or denoting a low vocal or instrumental range                                                 0.00176\n    9  bass%1:10:00::       the lowest adult male singing voice                                                                  0.00157\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  She went to the [TGT] bank [TGT] to deposit money\n"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"143f340911c64feea1be2cce45339522"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Progress:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c4584ae11a48b9a11a2c3d4de01ae0"}},"metadata":{}},{"name":"stdout","text":"\nPredictions:\n  No.  Sense key       Definition                                                                                                                           Score\n-----  --------------  ---------------------------------------------------------------------------------------------------------------------------------  -------\n    1  bank%1:14:00::  a financial institution that accepts deposits and channels the money into lending activities                                       0.57912\n    2  bank%1:06:00::  a building in which the business of banking transacted                                                                             0.41623\n    3  bank%2:40:00::  put into a bank account                                                                                                            0.00458\n    4  bank%2:40:02::  do business with a bank or keep an account at a bank                                                                               6e-05\n    5  bank%2:40:01::  be in the banking business                                                                                                         0\n    6  bank%2:35:00::  enclose with a bank                                                                                                                0\n    7  bank%1:06:01::  a container (usually with a slot in the top) for keeping money at home                                                             0\n    8  bank%1:21:01::  the funds held by a gambling house or the dealer in some gambling games                                                            0\n    9  bank%1:21:00::  a supply or stock held in reserve for future use (especially in emergencies)                                                       0\n   10  bank%1:14:01::  an arrangement of similar objects in a row or in tiers                                                                             0\n   11  bank%2:40:03::  act as the banker in a game or in gambling                                                                                         0\n   12  bank%1:17:01::  sloping land (especially the slope beside a body of water)                                                                         0\n   13  bank%1:17:02::  a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force  0\n   14  bank%2:38:00::  tip laterally                                                                                                                      0\n   15  bank%1:17:00::  a long ridge or pile                                                                                                               0\n   16  bank%2:31:02::  have confidence or faith in                                                                                                        0\n   17  bank%1:04:00::  a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)                                     0\n   18  bank%2:35:01::  cover with ashes so to control the rate of burning                                                                                 0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  We have a [TGT] date [TGT] for dinner tomorrow night\n"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"788f5577b2c943ed8b7d11ddf65539b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Progress:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1956f9cceb2e47cfb68b77144b6610b5"}},"metadata":{}},{"name":"stdout","text":"\nPredictions:\n  No.  Sense key       Definition                                                                                                 Score\n-----  --------------  -------------------------------------------------------------------------------------------------------  -------\n    1  date%2:41:00::  go on a date with                                                                                         0.9923\n    2  date%2:31:00::  assign a date to; determine the (probable) date of                                                        0.0047\n    3  date%1:14:00::  a meeting arranged in advance                                                                             0.0029\n    4  date%1:18:00::  a participant in a date                                                                                   4e-05\n    5  date%1:28:00::  the specified day of the month                                                                            3e-05\n    6  date%1:28:05::  a particular day specified as the time something happens                                                  3e-05\n    7  date%2:41:01::  date regularly; have a steady relationship with                                                           0\n    8  date%1:28:02::  the present                                                                                               0\n    9  date%2:31:01::  provide with a dateline; mark with a date                                                                 0\n   10  date%1:28:03::  the particular day, month, or year (usually according to the Gregorian calendar) that an event occurred   0\n   11  date%1:28:04::  a particular but unspecified point in time                                                                0\n   12  date%2:31:02::  stamp with a date                                                                                         0\n   13  date%1:13:00::  sweet edible fruit of the date palm with a single long woody seed                                         0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  I will have a [TGT]meeting[TGT] this night\n"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ead6d7e0e904d3398d75a8d61bb2ec2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Progress:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d15eafcc96b43718327d0876b6295b4"}},"metadata":{}},{"name":"stdout","text":"\nPredictions:\n  No.  Sense key          Definition                                                         Score\n-----  -----------------  ---------------------------------------------------------------  -------\n    1  meeting%1:14:00::  a formally arranged gathering                                    0.95642\n    2  meeting%1:11:00::  a casual or unexpected convergence                               0.04358\n    3  meeting%1:04:00::  the social act of assembling for some common purpose             0\n    4  meeting%1:14:01::  a small informal social gathering                                0\n    5  meeting%1:04:02::  the act of joining together as one                               0\n    6  meeting%1:15:00::  a place where things merge or flow together (especially rivers)  0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  i go to [TGT]market[TGT]\n"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba76a0296aac448bbc415619f86f4e8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Progress:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb2760cc55243f5bbd1717884bb0820"}},"metadata":{}},{"name":"stdout","text":"\nPredictions:\n  No.  Sense key         Definition                                                                       Score\n-----  ----------------  -----------------------------------------------------------------------------  -------\n    1  market%1:04:00::  the world of commercial activity where goods and services are bought and sold  0.95843\n    2  market%1:14:00::  the customers for a particular product or service                              0.03516\n    3  market%1:06:00::  a marketplace where groceries are sold                                         0.00627\n    4  market%2:40:00::  engage in the commercial promotion, sale, or distribution of                   0.0001\n    5  market%1:06:01::  an area in a town where a public mercantile establishment is set up            2e-05\n    6  market%2:40:05::  buy household supplies                                                         2e-05\n    7  market%2:40:01::  deal in a market                                                               0\n    8  market%2:30:00::  make commercial                                                                0\n    9  market%1:14:01::  the securities markets in the aggregate                                        0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  the [TGT] date [TGT] of the meeting was postponed.\n"},{"output_type":"display_data","data":{"text/plain":"Conversion des données:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31424b688f6d4c8e83296d82f699e48d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Progress:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c660c31a1ae4e158b8ad4026c3900be"}},"metadata":{}},{"name":"stdout","text":"\nPredictions:\n  No.  Sense key       Definition                                                                                                 Score\n-----  --------------  -------------------------------------------------------------------------------------------------------  -------\n    1  date%1:28:00::  the specified day of the month                                                                           0.79789\n    2  date%2:31:00::  assign a date to; determine the (probable) date of                                                       0.20169\n    3  date%1:28:05::  a particular day specified as the time something happens                                                 0.00038\n    4  date%1:28:02::  the present                                                                                              2e-05\n    5  date%1:28:04::  a particular but unspecified point in time                                                               1e-05\n    6  date%1:28:03::  the particular day, month, or year (usually according to the Gregorian calendar) that an event occurred  0\n    7  date%1:14:00::  a meeting arranged in advance                                                                            0\n    8  date%1:18:00::  a participant in a date                                                                                  0\n    9  date%2:31:01::  provide with a dateline; mark with a date                                                                0\n   10  date%2:41:00::  go on a date with                                                                                        0\n   11  date%2:31:02::  stamp with a date                                                                                        0\n   12  date%1:13:00::  sweet edible fruit of the date palm with a single long woody seed                                        0\n   13  date%2:41:01::  date regularly; have a steady relationship with                                                          0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\n>  quit\n"},{"name":"stdout","text":"\nIncorrect input format. Please try again.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;66;03m# for i, (sense_key, definition, score) in enumerate(predictions):\u001b[39;00m\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;66;03m#     # print(f\"  {i + 1:>3}. sense key: {sense_key:<15} score: {score:<8.5f} definition: {definition}\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[51], line 11\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEnter a sentence with an ambiguous word surrounded by [TGT] tokens\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m get_predictions(model, tokenizer, sentence)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m predictions:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":51}]}