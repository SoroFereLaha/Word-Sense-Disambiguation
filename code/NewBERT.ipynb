{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Bienvenue dans Colaboratory","provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10208449,"sourceType":"datasetVersion","datasetId":6309077}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nimport math\nimport random\nimport logging\nimport argparse\nimport itertools\nfrom collections import namedtuple\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.metrics import precision_score\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import (\n    BertModel, BertConfig, BertPreTrainedModel,\n    BertTokenizer, AdamW, get_linear_schedule_with_warmup\n)","metadata":{"id":"pesTpjUR0fqj","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:55:12.962988Z","iopub.execute_input":"2024-12-16T04:55:12.963645Z","iopub.status.idle":"2024-12-16T04:55:29.017016Z","shell.execute_reply.started":"2024-12-16T04:55:12.963611Z","shell.execute_reply":"2024-12-16T04:55:29.016361Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Configuration des journaux\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Structures de données\nGlossSelectionRecord = namedtuple(\"GlossSelectionRecord\", [\"guid\", \"sentence\", \"sense_keys\", \"glosses\", \"targets\"])\nBertInput = namedtuple(\"BertInput\", [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_id\"])","metadata":{"id":"_yaIyehX1MSp","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:55:36.582563Z","iopub.execute_input":"2024-12-16T04:55:36.583411Z","iopub.status.idle":"2024-12-16T04:55:36.588183Z","shell.execute_reply.started":"2024-12-16T04:55:36.583376Z","shell.execute_reply":"2024-12-16T04:55:36.587282Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class WSDDataset(Dataset):\n    def __init__(self, features):\n        self.features = features\n\n    def __getitem__(self, index):\n        return self.features[index]\n\n    def __len__(self):\n        return len(self.features)","metadata":{"id":"ChU6eVKR1ShA","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:55:40.135201Z","iopub.execute_input":"2024-12-16T04:55:40.135982Z","iopub.status.idle":"2024-12-16T04:55:40.140189Z","shell.execute_reply.started":"2024-12-16T04:55:40.135948Z","shell.execute_reply":"2024-12-16T04:55:40.139229Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class BertWSD(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n        self.ranking_linear = torch.nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n\ndef _compute_weighted_loss(loss, weighting_factor):\n    \"\"\"Calcul d'une perte pondérée\"\"\"\n    squared_factor = weighting_factor ** 2\n    return 1 / (2 * squared_factor) * loss + math.log(1 + squared_factor)\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Tronque une paire de séquences à la longueur maximale\"\"\"\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","metadata":{"id":"m5SyyrL91MYW","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:55:48.471749Z","iopub.execute_input":"2024-12-16T04:55:48.472059Z","iopub.status.idle":"2024-12-16T04:55:48.478956Z","shell.execute_reply.started":"2024-12-16T04:55:48.472033Z","shell.execute_reply":"2024-12-16T04:55:48.477875Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_dataset(\n    csv_path, \n    tokenizer, \n    max_sequence_length, \n    max_samples=None\n):\n    \"\"\"\n    Charge le jeu de données à partir d'un fichier CSV avec option de sous-échantillonnage.\n    \n    Args:\n        csv_path (str): Chemin vers le fichier CSV\n        tokenizer (BertTokenizer): Tokenizer BERT\n        max_sequence_length (int): Longueur maximale des séquences\n        max_samples (int, optional): Nombre maximal d'échantillons à charger\n    \n    Returns:\n        WSDDataset: Jeu de données pour l'entraînement ou l'évaluation\n    \"\"\"\n    def _deserialize_csv_record(row):\n        return GlossSelectionRecord(\n            row[0],  # guid\n            row[1],  # sentence\n            eval(row[2]),  # sense_keys\n            eval(row[3]),  # glosses\n            [int(t) for t in eval(row[4])]  # targets\n        )\n\n    def _create_records_from_csv(csv_path, deserialize_fn, max_samples=None):\n        \"\"\"\n        Crée des enregistrements à partir d'un fichier CSV avec sous-échantillonnage.\n        \n        Args:\n            csv_path (str): Chemin du fichier CSV\n            deserialize_fn (callable): Fonction de désérialisation\n            max_samples (int, optional): Nombre maximal d'échantillons\n        \n        Returns:\n            list: Liste d'enregistrements\n        \"\"\"\n        records = []\n        with open(csv_path, 'r', encoding='utf-8', newline='') as f:\n            reader = csv.reader(f)\n            next(reader)  # Ignorer l'en-tête\n            \n            # Utiliser itertools pour limiter les échantillons\n            for row in itertools.islice(reader, max_samples):\n                records.append(deserialize_fn(row))\n        \n        return records\n\n    # Charger les enregistrements avec limitation optionnelle\n    records = _create_records_from_csv(\n        csv_path, \n        _deserialize_csv_record, \n        max_samples\n    )\n    \n    # Convertir en features\n    features = _create_features_from_records(\n        records, \n        max_sequence_length, \n        tokenizer\n    )\n    \n    # Log du nombre d'échantillons chargés\n    logger.info(f\"Chargé {len(features)} échantillons depuis {csv_path}\")\n    \n    return WSDDataset(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:55:52.799862Z","iopub.execute_input":"2024-12-16T04:55:52.800553Z","iopub.status.idle":"2024-12-16T04:55:52.807975Z","shell.execute_reply.started":"2024-12-16T04:55:52.800518Z","shell.execute_reply":"2024-12-16T04:55:52.807033Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def _create_features_from_records(records, max_seq_length, tokenizer):\n    \"\"\"Convertit les enregistrements en features pour BERT\"\"\"\n    features = []\n    for record in tqdm(records, desc=\"Conversion des données\"):\n        tokens_a = tokenizer.tokenize(record.sentence)\n        sequences = [(gloss, 1 if i in record.targets else 0) for i, gloss in enumerate(record.glosses)]\n\n        pairs = []\n        for seq, label in sequences:\n            tokens_b = tokenizer.tokenize(seq)\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n\n            tokens = tokens_a + ['[SEP]']\n            segment_ids = [0] * len(tokens)\n\n            tokens += tokens_b + ['[SEP]']\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n            tokens = ['[CLS]'] + tokens\n            segment_ids = [0] + segment_ids\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n            input_mask = [1] * len(input_ids)\n\n            padding_length = max_seq_length - len(input_ids)\n            input_ids += [0] * padding_length\n            input_mask += [0] * padding_length\n            segment_ids += [0] * padding_length\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            pairs.append(\n                BertInput(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label)\n            )\n\n        features.append(pairs)\n\n    return features\n\ndef collate_batch(batch):\n    \"\"\"Regroupe les lots de données\"\"\"\n    max_seq_length = len(batch[0][0].input_ids)\n\n    collated = []\n    for sub_batch in batch:\n        batch_size = len(sub_batch)\n        sub_collated = [torch.zeros([batch_size, max_seq_length], dtype=torch.long) for _ in range(3)] + \\\n                       [torch.zeros([batch_size], dtype=torch.long)]\n\n        for i, bert_input in enumerate(sub_batch):\n            sub_collated[0][i] = torch.tensor(bert_input.input_ids, dtype=torch.long)\n            sub_collated[1][i] = torch.tensor(bert_input.input_mask, dtype=torch.long)\n            sub_collated[2][i] = torch.tensor(bert_input.segment_ids, dtype=torch.long)\n            sub_collated[3][i] = torch.tensor(bert_input.label_id, dtype=torch.long)\n\n        collated.append(sub_collated)\n\n    return collated\n\ndef forward_gloss_selection(model, batches, device):\n    \"\"\"Effectue une passe avant pour la sélection de gloses\"\"\"\n    batch_loss = 0\n    logits_list = []\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    for batch in batches:\n        batch = tuple(t.to(device) for t in batch)\n        outputs = model.bert(input_ids=batch[0], attention_mask=batch[1], token_type_ids=batch[2])\n        hidden_state = model.dropout(outputs[1])\n\n        logits = model.ranking_linear(hidden_state).squeeze(-1)\n        labels = torch.max(batch[3], -1).indices.detach()\n        batch_loss += loss_fn(logits.unsqueeze(dim=0), labels.unsqueeze(dim=-1))\n        logits_list.append(logits)\n\n    loss = batch_loss / len(batches)\n    return loss, logits_list","metadata":{"id":"ZEKmdd0J1Sj6","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:55:57.510885Z","iopub.execute_input":"2024-12-16T04:55:57.511676Z","iopub.status.idle":"2024-12-16T04:55:57.525412Z","shell.execute_reply.started":"2024-12-16T04:55:57.511642Z","shell.execute_reply":"2024-12-16T04:55:57.524406Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_wsd(train_path, eval_path, output_dir='./results',max_train_samples=None, max_eval_samples=None):\n    \"\"\"\n    Entraînement du modèle de sélection de gloses avec sous-échantillonnage.\n    \n    Args:\n        train_path (str): Chemin du fichier CSV d'entraînement\n        eval_path (str): Chemin du fichier CSV d'évaluation\n        output_dir (str, optional): Répertoire de sauvegarde du modèle\n        max_train_samples (int, optional): Nombre maximal d'échantillons d'entraînement\n        max_eval_samples (int, optional): Nombre maximal d'échantillons d'évaluation\n    \"\"\"\n    # Configuration\n    max_seq_length = 128\n    batch_size = 8\n    num_train_epochs = 3\n    learning_rate = 5e-5\n    seed = 42\n\n    # Configuration du seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # Périphérique\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    logger.info(f\"Utilisation du périphérique : {device}\")\n\n    # Modèle et Tokenizer\n    model_name = 'bert-base-cased'\n    config = BertConfig.from_pretrained(model_name, num_labels=2)\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    model = BertWSD.from_pretrained(model_name, config=config)\n\n    # Ajout du token spécial\n    if '[TGT]' not in tokenizer.additional_special_tokens:\n        tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n        model.resize_token_embeddings(len(tokenizer))\n\n    model.to(device)\n    \n    # Chargement des données avec sous-échantillonnage optionnel\n    train_dataset = load_dataset(\n        train_path, \n        tokenizer, \n        max_sequence_length=max_seq_length, \n        max_samples=max_train_samples\n    )\n\n    train_sampler = RandomSampler(train_dataset)\n    \n    train_dataloader = DataLoader(\n        train_dataset, \n        sampler=train_sampler, \n        batch_size=batch_size, \n        collate_fn=collate_batch\n    )\n    \n    # Optionnel : charger le jeu de données d'évaluation\n    if max_eval_samples is not None:\n        eval_dataset = load_dataset(\n            eval_path, \n            tokenizer, \n            max_sequence_length=max_seq_length, \n            max_samples=max_eval_samples\n        )\n    \n    # Préparation de l'optimiseur\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n            'weight_decay': 0.01\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n            'weight_decay': 0.0\n        }\n    ]\n\n    total_steps = len(train_dataloader) * num_train_epochs\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=total_steps\n    )\n    \n    logger.info(\"🚀 Entrainement Model Word Sense Disambiguation \")\n    logger.info(f\"Nombre total de lots de formatio (Batches): {len(train_dataloader)}\")\n    logger.info(f\"Device: {device}\")\n    \n    # Boucle d'entraînement\n    for epoch in range(num_train_epochs):\n        model.train()\n        total_loss = 0\n\n        # Track predictions and labels for precision calculation\n        all_preds = []\n        all_labels = []\n\n        # Create epoch progress bar\n        with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_train_epochs}\", \n                          unit=\"batch\", colour=\"green\") as epoch_iterator:\n        \n            start_time = time.time()\n    \n            for step, batches in enumerate(epoch_iterator):\n                loss, logits_list = forward_gloss_selection(model, batches, device)\n\n                # Collect predictions and true labels\n                for batch_logits, batch in zip(logits_list, batches):\n                    # Convert logits to predictions\n                    preds = (batch_logits > 0.5).cpu().numpy().astype(int)\n                    \n                    # Get true labels\n                    labels = batch[3].cpu().numpy()\n                    \n                    all_preds.extend(preds)\n                    all_labels.extend(labels)\n    \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n                optimizer.step()\n                scheduler.step()\n    \n                total_loss += loss.item()\n\n                # Calculate precision\n                try:\n                    precision = precision_score(all_labels, all_preds, zero_division=0)\n                except:\n                    precision = 0\n    \n                 # Update progress bar with real-time metrics\n                epoch_iterator.set_postfix({\n                    'Loss': f'{loss.item():.4f}', \n                    'Avg Loss': f'{total_loss/(step+1):.4f}',\n                    'Precision': f'{precision:.4f}',\n                    'Learning Rate': f'{scheduler.get_last_lr()[0]:.6f}'\n                })\n    \n                # End of epoch summary\n                epoch_duration = time.time() - start_time\n                # End of epoch summary\n                final_precision = precision_score(all_labels, all_preds, zero_division=0)\n                logger.info(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds. \"\n                            f\"Average Loss: {total_loss/len(train_dataloader):.4f}\"\n                            f\"Precision: {final_precision:.4f}\")\n    \n    \n                if step % 100 == 0:\n                    logger.info(f\"Époque {epoch}, Étape {step}, Perte : {loss.item()}\")\n\n    logger.info(\"✅ Entrainement terminé avec succès\")\n    # Sauvegarde du modèle\n    os.makedirs(output_dir, exist_ok=True)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    logger.info(f\"Modèle entraîné et sauvegardé dans {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T04:56:06.819201Z","iopub.execute_input":"2024-12-16T04:56:06.819523Z","iopub.status.idle":"2024-12-16T04:56:06.834728Z","shell.execute_reply.started":"2024-12-16T04:56:06.819495Z","shell.execute_reply":"2024-12-16T04:56:06.833805Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Charger seulement 5000 échantillons pour un test rapide\ntrain_wsd(\n    train_path='/kaggle/input/dataset/corpus_dir-max_num_gloss5-augmented.csv', \n    eval_path='/kaggle/input/dataset/semeval2007-max_num_gloss5-augmented.csv', \n    max_train_samples=50000, #40000==4heure      # Limiter à 5000 échantillons\n    max_eval_samples=10000  #8000==20%      # Limiter à 1000 échantillons d'évaluation\n)\n\n# Charger tous les échantillons (comportement par défaut)\n# train_wsd(train_path='/chemin/vers/train.csv', eval_path='/chemin/vers/eval.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}