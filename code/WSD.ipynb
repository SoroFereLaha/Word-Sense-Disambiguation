{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71cd360",
   "metadata": {},
   "source": [
    "# Étape 1 : Analyse des phrases dans le fichier XML\n",
    "Les fichiers .data.xml contiennent des phrases ou des contextes annotés, tandis que les fichiers .gold.key.txt associent les mots cibles à leurs sens (par exemple, des BabelNet IDs).\n",
    "\n",
    "### Extrait des fichiers XML : \n",
    "Utilisation de la bibliothèque xml.etree.ElementTree pour extraire les phrases et mots annotés : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efd4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin dataset en français\n",
    "french_data_path = r\"C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\french-data\\training_datasets_fr\\semcor_fr\\semcor_fr.data.xml\"\n",
    "\n",
    "# chemin dataset en anglais\n",
    "english_data_path = r\"C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\english-data\\training_datasets_en\\semcor_en\\semcor_en.data.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcacba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_data(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    sentences = []\n",
    "\n",
    "    for text in root.findall('.//text'):  # Parcourt les textes\n",
    "        for sentence in text.findall('.//sentence'):  # Parcourt les phrases\n",
    "            words = []\n",
    "            # Ne garde que les mots annotés comme <instance>\n",
    "            for instance in sentence.findall('.//instance'):  # Trouve les mots annotés \"instance\"\n",
    "                words.append(instance.text)\n",
    "            sentences.append(\" \".join(words))  # Concatène les mots annotés\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f232eaf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long been reviewed objectives benefit service program', 'permitted become giveaway program rather has goal improved employee morale consequently increased productivity', 'effort make assess results program', 'measure reduced absenteeism turnover accidents grievances improved quality output', 'set specific objectives employee publication']\n"
     ]
    }
   ],
   "source": [
    "# Test avec le fichier XML\n",
    "sentences = parse_data(english_data_path)\n",
    "\n",
    "print(sentences[:5])  # Affiche les 5 premières phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b293fb4",
   "metadata": {},
   "source": [
    "# Étape 2 : Association des mots annotés avec leurs identifiants de sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20349f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gold_keys(file_path):\n",
    "    gold_keys = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                instance_id = parts[0]\n",
    "                # Si plusieurs sens sont associés à l'instance, les ajouter dans une liste\n",
    "                sense_ids = parts[1:]\n",
    "                gold_keys[instance_id] = sense_ids\n",
    "            else:\n",
    "                print(f\"Ligne ignorée (format incorrect): {line}\")  # Affiche une ligne mal formatée\n",
    "    return gold_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2e85ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d000.s000.t000', ['bn:00106124a']), ('d000.s000.t001', ['bn:00083181v']), ('d000.s000.t002', ['bn:00092618v']), ('d000.s000.t003', ['bn:00002179n']), ('d000.s000.t004', ['bn:00009904n'])]\n"
     ]
    }
   ],
   "source": [
    "# Test avec le fichier gold key\n",
    "gold_key_path = r\"C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\English-data\\training_datasets_en\\semcor_en\\semcor_en.gold.key.txt\"\n",
    "gold_keys = parse_gold_keys(gold_key_path)\n",
    "\n",
    "print(list(gold_keys.items())[:5])  # Affiche les 5 premiers mappings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75dbf82",
   "metadata": {},
   "source": [
    "# Étape 3 : Associer les phrases avec leurs annotations \n",
    "Nous combinons les phrases et les identifiants de sens en utilisant les informations du fichier gold key :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135dd6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_annotations(file_path, gold_keys):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = []\n",
    "\n",
    "    for text in root.findall('.//text'):\n",
    "        for sentence in text.findall('.//sentence'):\n",
    "            sentence_id = sentence.attrib['id']  # ID de la phrase\n",
    "            annotated_words = []\n",
    "            for instance in sentence.findall('.//instance'):  # Parcourt les instances annotées\n",
    "                instance_id = instance.attrib['id']  # ID de l'instance\n",
    "                word = instance.text\n",
    "                sense_id = gold_keys.get(instance_id, \"unknown\")  # Récupère l'identifiant de sens\n",
    "                annotated_words.append((word, sense_id))\n",
    "            annotations.append((sentence_id, annotated_words))\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211e07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: d000.s000\n",
      "  Word: long, Sense ID: ['bn:00106124a']\n",
      "  Word: been, Sense ID: ['bn:00083181v']\n",
      "  Word: reviewed, Sense ID: ['bn:00092618v']\n",
      "  Word: objectives, Sense ID: ['bn:00002179n']\n",
      "  Word: benefit, Sense ID: ['bn:00009904n']\n",
      "  Word: service, Sense ID: ['bn:00070654n']\n",
      "  Word: program, Sense ID: ['bn:00064646n']\n",
      "Sentence ID: d000.s001\n",
      "  Word: permitted, Sense ID: ['bn:00082536v']\n",
      "  Word: become, Sense ID: ['bn:00083294v']\n",
      "  Word: giveaway, Sense ID: ['bn:00040564n']\n",
      "  Word: program, Sense ID: ['bn:00064646n']\n",
      "  Word: rather, Sense ID: ['bn:00115972r']\n",
      "  Word: has, Sense ID: ['bn:00088078v']\n",
      "  Word: goal, Sense ID: ['bn:00030721n']\n",
      "  Word: improved, Sense ID: ['bn:00104710a']\n",
      "  Word: employee, Sense ID: ['bn:00030618n']\n",
      "  Word: morale, Sense ID: ['bn:00055852n']\n",
      "  Word: consequently, Sense ID: ['bn:00114106r']\n",
      "  Word: increased, Sense ID: ['bn:00104887a']\n",
      "  Word: productivity, Sense ID: ['bn:00064598n']\n",
      "Sentence ID: d000.s002\n",
      "  Word: effort, Sense ID: ['bn:00007011n']\n",
      "  Word: make, Sense ID: ['bn:00087106v']\n",
      "  Word: assess, Sense ID: ['bn:00082720v']\n",
      "  Word: results, Sense ID: ['bn:00034520n']\n",
      "  Word: program, Sense ID: ['bn:00064646n']\n",
      "Sentence ID: d000.s003\n",
      "  Word: measure, Sense ID: ['bn:00082720v']\n",
      "  Word: reduced, Sense ID: ['bn:00101030a']\n",
      "  Word: absenteeism, Sense ID: ['bn:00000435n']\n",
      "  Word: turnover, Sense ID: ['bn:00030623n']\n",
      "  Word: accidents, Sense ID: ['bn:00000690n']\n",
      "  Word: grievances, Sense ID: ['bn:00041786n']\n",
      "  Word: improved, Sense ID: ['bn:00104711a']\n",
      "  Word: quality, Sense ID: ['bn:00065538n']\n",
      "  Word: output, Sense ID: ['bn:00059788n']\n",
      "Sentence ID: d000.s004\n",
      "  Word: set, Sense ID: ['bn:00086468v']\n",
      "  Word: specific, Sense ID: ['bn:00111001a']\n",
      "  Word: objectives, Sense ID: ['bn:00002179n']\n",
      "  Word: employee, Sense ID: ['bn:00030618n']\n",
      "  Word: publication, Sense ID: ['bn:00065106n']\n"
     ]
    }
   ],
   "source": [
    "# Associer annotations\n",
    "annotations = associate_annotations(english_data_path, gold_keys)\n",
    "\n",
    "# Afficher les 5 premières annotations\n",
    "for sentence_id, annotated_words in annotations[:5]:\n",
    "    print(f\"Sentence ID: {sentence_id}\")\n",
    "    for word, sense_id in annotated_words:\n",
    "        print(f\"  Word: {word}, Sense ID: {sense_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35779731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37176\n",
      "37176\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df43fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase : long been reviewed objectives benefit service program\n",
      "Annotations : ('d000.s000', [('long', ['bn:00106124a']), ('been', ['bn:00083181v']), ('reviewed', ['bn:00092618v']), ('objectives', ['bn:00002179n']), ('benefit', ['bn:00009904n']), ('service', ['bn:00070654n']), ('program', ['bn:00064646n'])])\n",
      "Phrase : permitted become giveaway program rather has goal improved employee morale consequently increased productivity\n",
      "Annotations : ('d000.s001', [('permitted', ['bn:00082536v']), ('become', ['bn:00083294v']), ('giveaway', ['bn:00040564n']), ('program', ['bn:00064646n']), ('rather', ['bn:00115972r']), ('has', ['bn:00088078v']), ('goal', ['bn:00030721n']), ('improved', ['bn:00104710a']), ('employee', ['bn:00030618n']), ('morale', ['bn:00055852n']), ('consequently', ['bn:00114106r']), ('increased', ['bn:00104887a']), ('productivity', ['bn:00064598n'])])\n",
      "Phrase : effort make assess results program\n",
      "Annotations : ('d000.s002', [('effort', ['bn:00007011n']), ('make', ['bn:00087106v']), ('assess', ['bn:00082720v']), ('results', ['bn:00034520n']), ('program', ['bn:00064646n'])])\n",
      "Phrase : measure reduced absenteeism turnover accidents grievances improved quality output\n",
      "Annotations : ('d000.s003', [('measure', ['bn:00082720v']), ('reduced', ['bn:00101030a']), ('absenteeism', ['bn:00000435n']), ('turnover', ['bn:00030623n']), ('accidents', ['bn:00000690n']), ('grievances', ['bn:00041786n']), ('improved', ['bn:00104711a']), ('quality', ['bn:00065538n']), ('output', ['bn:00059788n'])])\n",
      "Phrase : set specific objectives employee publication\n",
      "Annotations : ('d000.s004', [('set', ['bn:00086468v']), ('specific', ['bn:00111001a']), ('objectives', ['bn:00002179n']), ('employee', ['bn:00030618n']), ('publication', ['bn:00065106n'])])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Phrase : {sentences[i]}\")\n",
    "    print(f\"Annotations : {annotations[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189910d",
   "metadata": {},
   "source": [
    "# Resultats\n",
    "\n",
    "Extraction des données XML : Les phrases ont été correctement extraites à partir du fichier XML, en se concentrant sur les instances annotées.\n",
    "Gestion des clés dorées (gold keys) : Les clés dorées (celles correspondant aux sens des mots) ont été extraites et plusieurs identifiants de sens sont gérés correctement pour une même instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d4f16",
   "metadata": {},
   "source": [
    "# Prochaine étape : Désambiguïsation des mots avec un modèle Transformers\n",
    "Maintenant que les données sont correctement extraites et annotées, la prochaine étape serait d'appliquer un modèle de désambiguïsation de sens (Word Sense Disambiguation - WSD) utilisant un modèle de type Transformers.\n",
    "\n",
    "Pour cela, on peux utiliser des modèles pré-entraînés comme BERT, RoBERTa (version améliorés de BERT), ou XLM-R (version multilingue de BERT) qui sont bien adaptés pour les tâches de désambiguïsation des mots. Ces modèles sont disponibles via la bibliothèque transformers de Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69249f65",
   "metadata": {},
   "source": [
    "# BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfef71",
   "metadata": {},
   "source": [
    "# RoBERTa ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d0405",
   "metadata": {},
   "source": [
    "#### Conversion en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3625d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_to_json(sentences_with_annotations, output_path):\n",
    "    \"\"\"\n",
    "    Exporte les données des phrases avec annotations dans un fichier JSON.\n",
    "    \n",
    "    :param sentences_with_annotations: Liste contenant des tuples (ID de phrase, annotations).\n",
    "    :param output_path: Le chemin du fichier de sortie pour le fichier JSON.\n",
    "    \"\"\"\n",
    "    sentences_data = []\n",
    "    for sentence_data in sentences_with_annotations:\n",
    "        sentence_id = sentence_data[0]  # L'ID de la phrase se trouve à l'index 0\n",
    "        annotations = [\n",
    "            [annotation[0], annotation[1]]  # Le mot est à l'index 0 et les sens à l'index 1\n",
    "            for annotation in sentence_data[1]\n",
    "        ]\n",
    "        sentences_data.append([sentence_id, annotations])\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(sentences_data, json_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les données ont été exportées en JSON vers {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28ec628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ont été exportées en JSON vers C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\english-data\\english_wsd_dataset.json\n"
     ]
    }
   ],
   "source": [
    "output_path_json = r\"C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\english-data\\english_wsd_dataset.json\"\n",
    "export_to_json(annotations, output_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "280410b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>[[long, [bn:00106124a]], [been, [bn:00083181v]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s001</td>\n",
       "      <td>[[permitted, [bn:00082536v]], [become, [bn:000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s002</td>\n",
       "      <td>[[effort, [bn:00007011n]], [make, [bn:00087106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s003</td>\n",
       "      <td>[[measure, [bn:00082720v]], [reduced, [bn:0010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s004</td>\n",
       "      <td>[[set, [bn:00086468v]], [specific, [bn:0011100...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                                                  1\n",
       "0  d000.s000  [[long, [bn:00106124a]], [been, [bn:00083181v]...\n",
       "1  d000.s001  [[permitted, [bn:00082536v]], [become, [bn:000...\n",
       "2  d000.s002  [[effort, [bn:00007011n]], [make, [bn:00087106...\n",
       "3  d000.s003  [[measure, [bn:00082720v]], [reduced, [bn:0010...\n",
       "4  d000.s004  [[set, [bn:00086468v]], [specific, [bn:0011100..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_json(output_path_json).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81e957",
   "metadata": {},
   "source": [
    "#### Conversion en pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01518579-f798-45d3-9b70-87cd601ebda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_dataframe(sentences_with_annotations, output_path):\n",
    "    \"\"\"\n",
    "    Exporte les données des phrases avec annotations dans un fichier CSV.\n",
    "    \n",
    "    :param sentences_with_annotations: Liste contenant des tuples (ID de phrase, annotations).\n",
    "    :param output_path: Le chemin du fichier de sortie pour le fichier CSV.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for sentence_data in sentences_with_annotations:\n",
    "        sentence_id = sentence_data[0]  # L'ID de la phrase se trouve à l'index 0\n",
    "        for annotation in sentence_data[1]:  # Les annotations sont à l'index 1\n",
    "            word = annotation[0]  # Le mot est à l'index 0 de l'annotation\n",
    "            sense_id = ', '.join(annotation[1])  # Les sens sont à l'index 1 et doivent être joints par une virgule\n",
    "            data.append([sentence_id, word, sense_id])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['sentence', 'word', 'sense'])\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Les données ont été exportées en CSV vers {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9708c5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ont été exportées en CSV vers C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\english-data\\english_wsd_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "output_path_csv = r\"C:\\Users\\pc\\Downloads\\BIBDA\\S3\\Analyse de Sentiments et Text Mining\\english-data\\english_wsd_dataset.csv\"\n",
    "export_to_dataframe(annotations, output_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9edf1509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>long</td>\n",
       "      <td>bn:00106124a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>been</td>\n",
       "      <td>bn:00083181v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>bn:00092618v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>objectives</td>\n",
       "      <td>bn:00002179n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>benefit</td>\n",
       "      <td>bn:00009904n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence        word         sense\n",
       "0  d000.s000        long  bn:00106124a\n",
       "1  d000.s000        been  bn:00083181v\n",
       "2  d000.s000    reviewed  bn:00092618v\n",
       "3  d000.s000  objectives  bn:00002179n\n",
       "4  d000.s000     benefit  bn:00009904n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_path_csv)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc323e-0cb5-4aa6-abdb-f712eedeca84",
   "metadata": {},
   "source": [
    "# Vérification des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae8131f0-70a6-45b8-8564-7c1b6047fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Vérification des valeurs manquantes\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Vérification des doublons\n",
    "duplicates = df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33191199-7073-43ef-afb9-01a853fb0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification du format des Sense IDs\n",
    "def is_valid_sense_id(sense_id):\n",
    "    pattern = r'^bn:\\d{8}[a-z]$'\n",
    "    return bool(re.match(pattern, sense_id))\n",
    "\n",
    "invalid_sense_ids = df[~df['sense'].apply(is_valid_sense_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0c7c078-58ff-487b-88cb-b8fbbb1250f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé des vérifications :\n",
      "\n",
      "Valeurs manquantes par colonne :\n",
      "sentence    0\n",
      "word        1\n",
      "sense       0\n",
      "dtype: int64\n",
      "\n",
      "Nombre de doublons : 5621\n",
      "\n",
      "Sense IDs invalides :\n",
      "         sentence      word                       sense\n",
      "72      d000.s010   service  bn:00070653n, bn:00070652n\n",
      "371     d000.s046    public  bn:00109212a, bn:00109211a\n",
      "1102    d001.s006     break  bn:00083902v, bn:00083904v\n",
      "1159    d001.s016  handling  bn:00084527v, bn:00087116v\n",
      "1196    d001.s023     cause  bn:00016845n, bn:00016850n\n",
      "...           ...       ...                         ...\n",
      "222803  d339.s009    caring  bn:00084527v, bn:00084524v\n",
      "223324  d340.s122       lay  bn:00090322v, bn:00090321v\n",
      "224476  d345.s107    called  bn:00084385v, bn:00084386v\n",
      "224498  d345.s116      call  bn:00084385v, bn:00084386v\n",
      "224618  d346.s042      show  bn:00083245v, bn:00086557v\n",
      "\n",
      "[656 rows x 3 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Résultats\n",
    "print(\"Résumé des vérifications :\\n\")\n",
    "print(f\"Valeurs manquantes par colonne :\\n{missing_values}\\n\")\n",
    "print(f\"Nombre de doublons : {duplicates}\\n\")\n",
    "if not invalid_sense_ids.empty:\n",
    "    print(f\"Sense IDs invalides :\\n{invalid_sense_ids}\\n\")\n",
    "else:\n",
    "    print(\"Tous les Sense IDs sont valides.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26409db6-b432-4a5c-ab85-b3a4c0f75ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python311\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.1)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.0 MB 8.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.0 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.3 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "614715f8-f67a-4ed7-8065-6a5c811f1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Charger le tokenizer pour le modèle souhaité\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c25bcebd-572c-490f-9792-b1d9774fa27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(df, tokenizer, max_len=128):\n",
    "    # Vérification des données\n",
    "    print(\"Aperçu des mots : \", df['word'].head())\n",
    "    print(\"Types des mots : \", df['word'].apply(type).unique())\n",
    "\n",
    "    # Convertir toutes les valeurs en chaînes, si nécessaire\n",
    "    df['word'] = df['word'].astype(str)\n",
    "\n",
    "    # Tokenization\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        df['word'].tolist(),  # Liste des mots\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Retourner les entrées tokenisées\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aaec12f4-e789-449f-bebe-fe7c1195c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des mots :  0          long\n",
      "1          been\n",
      "2      reviewed\n",
      "3    objectives\n",
      "4       benefit\n",
      "Name: word, dtype: object\n",
      "Types des mots :  [<class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_data_for_model(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb4baba8-49ed-4109-aa05-e8b137565176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:20])  # Aperçu des deux premières phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbc3b7-c982-4448-8bee-d4c804756118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
