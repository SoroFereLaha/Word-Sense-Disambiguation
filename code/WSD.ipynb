{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-09T14:52:09.330487Z",
     "iopub.status.busy": "2024-12-09T14:52:09.330140Z",
     "iopub.status.idle": "2024-12-09T14:52:11.039186Z",
     "shell.execute_reply": "2024-12-09T14:52:11.038465Z",
     "shell.execute_reply.started": "2024-12-09T14:52:09.330446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:34.952841Z",
     "iopub.status.busy": "2024-12-09T14:53:34.951955Z",
     "iopub.status.idle": "2024-12-09T14:53:34.957633Z",
     "shell.execute_reply": "2024-12-09T14:53:34.956832Z",
     "shell.execute_reply.started": "2024-12-09T14:53:34.952799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fichiers pour l'entraînement (choisissez un fichier parmi ceux de training_datasets_en)\n",
    "TRAIN_DATA_PATH = \"/kaggle/input/data-set/training_datasets_en/semcor_en/semcor_en.data.xml\"  # ou un autre dataset\n",
    "TRAIN_LABELS_PATH = \"/kaggle/input/data-set/training_datasets_en/semcor_en/semcor_en.gold.key.txt\"\n",
    "\n",
    "# Fichiers pour la validation\n",
    "DEV_DATA_PATH = \"/kaggle/input/data-set/evaluation_datasets_en/dev-en/dev-en.data.xml\"\n",
    "DEV_LABELS_PATH = \"/kaggle/input/data-set/evaluation_datasets_en/dev-en/dev-en.gold.key.txt\"\n",
    "\n",
    "# Fichiers pour les tests\n",
    "TEST_DATA_PATH = \"/kaggle/input/data-set/evaluation_datasets_en/test-en/test-en.data.xml\"\n",
    "TEST_LABELS_PATH = \"/kaggle/input/data-set/evaluation_datasets_en/test-en/test-en.gold.key.txt\"\n",
    "\n",
    "# Fichiers exemaple et glosses\n",
    "EXAMPLE_DATA_PATH = \"/kaggle/input/data-set/training_datasets_en/wngt_examples_en/wngt_examples_en.data.xml\"\n",
    "EXAMPLE_LABELS_PATH = \"/kaggle/input/data-set/training_datasets_en/wngt_examples_en/wngt_examples_en.gold.key.txt\"\n",
    "GLOSSES_DATA_PATH = \"/kaggle/input/data-set/training_datasets_en/wngt_glosses_en/wngt_glosses_en.data.xml\"\n",
    "GLOSSES_LABELS_PATH = \"/kaggle/input/data-set/training_datasets_en/wngt_glosses_en/wngt_glosses_en.gold.key.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:37.983865Z",
     "iopub.status.busy": "2024-12-09T14:53:37.983512Z",
     "iopub.status.idle": "2024-12-09T14:53:37.991037Z",
     "shell.execute_reply": "2024-12-09T14:53:37.990311Z",
     "shell.execute_reply.started": "2024-12-09T14:53:37.983837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def xml_to_dataframe(data_path, labels_path):\n",
    "    # Charger le fichier XML\n",
    "    tree = ET.parse(data_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    sentences = []\n",
    "    labels = {}\n",
    "\n",
    "    # Charger les labels depuis le fichier .gold.key.txt\n",
    "    with open(labels_path, 'r') as f:\n",
    "        for line in f:\n",
    "            instance_id, *label = line.strip().split()\n",
    "            labels[instance_id] = label  # Chaque instance_id peut avoir plusieurs labels\n",
    "\n",
    "    # Parcourir les phrases et extraire les tokens à désambiguïser\n",
    "    for text in root.findall(\".//text\"):\n",
    "        for sentence in text.findall(\".//sentence\"):\n",
    "            sentence_text = \"\"\n",
    "            for element in sentence:\n",
    "                if element.tag == \"wf\":\n",
    "                    # Mots normaux\n",
    "                    sentence_text += element.text + \" \"\n",
    "                elif element.tag == \"instance\":\n",
    "                    # Mots à désambiguïser\n",
    "                    word = element.text\n",
    "                    instance_id = element.attrib['id']\n",
    "                    lemma = element.attrib.get('lemma', \"\")\n",
    "                    pos = element.attrib.get('pos', \"\")\n",
    "                    sentence_text += word + \" \"\n",
    "\n",
    "                    # Ajouter au DataFrame\n",
    "                    sentences.append({\n",
    "                        \"sentence\": sentence_text.strip(),\n",
    "                        \"word\": word,\n",
    "                        \"lemma\": lemma,\n",
    "                        \"pos\": pos,\n",
    "                        \"instance_id\": instance_id,\n",
    "                        \"labels\": labels.get(instance_id, [])\n",
    "                    })\n",
    "    \n",
    "    # Convertir en DataFrame\n",
    "    return pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:41.909229Z",
     "iopub.status.busy": "2024-12-09T14:53:41.908539Z",
     "iopub.status.idle": "2024-12-09T14:53:46.171111Z",
     "shell.execute_reply": "2024-12-09T14:53:46.170387Z",
     "shell.execute_reply.started": "2024-12-09T14:53:41.909194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How long</td>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>[bn:00106124a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long has it been</td>\n",
       "      <td>been</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>[bn:00083181v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How long has it been since you reviewed</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>review</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>[bn:00092618v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>objectives</td>\n",
       "      <td>objective</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>[bn:00002179n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit</td>\n",
       "      <td>benefit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>[bn:00009904n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>service</td>\n",
       "      <td>service</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t005</td>\n",
       "      <td>[bn:00070654n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t006</td>\n",
       "      <td>[bn:00064646n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Have you permitted</td>\n",
       "      <td>permitted</td>\n",
       "      <td>permit</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s001.t000</td>\n",
       "      <td>[bn:00082536v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Have you permitted it to become</td>\n",
       "      <td>become</td>\n",
       "      <td>become</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s001.t001</td>\n",
       "      <td>[bn:00083294v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Have you permitted it to become a giveaway</td>\n",
       "      <td>giveaway</td>\n",
       "      <td>giveaway</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t002</td>\n",
       "      <td>[bn:00040564n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t003</td>\n",
       "      <td>[bn:00064646n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>rather</td>\n",
       "      <td>rather</td>\n",
       "      <td>ADV</td>\n",
       "      <td>d000.s001.t004</td>\n",
       "      <td>[bn:00115972r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>has</td>\n",
       "      <td>have</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s001.t005</td>\n",
       "      <td>[bn:00088078v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>goal</td>\n",
       "      <td>goal</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t006</td>\n",
       "      <td>[bn:00030721n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>improved</td>\n",
       "      <td>improved</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d000.s001.t007</td>\n",
       "      <td>[bn:00104710a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>employee</td>\n",
       "      <td>employee</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t008</td>\n",
       "      <td>[bn:00030618n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>morale</td>\n",
       "      <td>morale</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t009</td>\n",
       "      <td>[bn:00055852n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>consequently</td>\n",
       "      <td>consequently</td>\n",
       "      <td>ADV</td>\n",
       "      <td>d000.s001.t010</td>\n",
       "      <td>[bn:00114106r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>increased</td>\n",
       "      <td>increased</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d000.s001.t011</td>\n",
       "      <td>[bn:00104887a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>productivity</td>\n",
       "      <td>productivity</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t012</td>\n",
       "      <td>[bn:00064598n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What effort</td>\n",
       "      <td>effort</td>\n",
       "      <td>effort</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s002.t000</td>\n",
       "      <td>[bn:00007011n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What effort do you make</td>\n",
       "      <td>make</td>\n",
       "      <td>make</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s002.t001</td>\n",
       "      <td>[bn:00087106v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What effort do you make to assess</td>\n",
       "      <td>assess</td>\n",
       "      <td>assess</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s002.t002</td>\n",
       "      <td>[bn:00082720v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What effort do you make to assess results</td>\n",
       "      <td>results</td>\n",
       "      <td>result</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s002.t003</td>\n",
       "      <td>[bn:00034520n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What effort do you make to assess results of y...</td>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s002.t004</td>\n",
       "      <td>[bn:00064646n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Do you measure</td>\n",
       "      <td>measure</td>\n",
       "      <td>measure</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s003.t000</td>\n",
       "      <td>[bn:00082720v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Do you measure its relation to reduced</td>\n",
       "      <td>reduced</td>\n",
       "      <td>reduced</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d000.s003.t001</td>\n",
       "      <td>[bn:00101030a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Do you measure its relation to reduced absente...</td>\n",
       "      <td>absenteeism</td>\n",
       "      <td>absenteeism</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s003.t002</td>\n",
       "      <td>[bn:00000435n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Do you measure its relation to reduced absente...</td>\n",
       "      <td>turnover</td>\n",
       "      <td>turnover</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s003.t003</td>\n",
       "      <td>[bn:00030623n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Do you measure its relation to reduced absente...</td>\n",
       "      <td>accidents</td>\n",
       "      <td>accident</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s003.t004</td>\n",
       "      <td>[bn:00000690n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence          word  \\\n",
       "0                                            How long          long   \n",
       "1                                How long has it been          been   \n",
       "2             How long has it been since you reviewed      reviewed   \n",
       "3   How long has it been since you reviewed the ob...    objectives   \n",
       "4   How long has it been since you reviewed the ob...       benefit   \n",
       "5   How long has it been since you reviewed the ob...       service   \n",
       "6   How long has it been since you reviewed the ob...       program   \n",
       "7                                  Have you permitted     permitted   \n",
       "8                     Have you permitted it to become        become   \n",
       "9          Have you permitted it to become a giveaway      giveaway   \n",
       "10  Have you permitted it to become a giveaway pro...       program   \n",
       "11  Have you permitted it to become a giveaway pro...        rather   \n",
       "12  Have you permitted it to become a giveaway pro...           has   \n",
       "13  Have you permitted it to become a giveaway pro...          goal   \n",
       "14  Have you permitted it to become a giveaway pro...      improved   \n",
       "15  Have you permitted it to become a giveaway pro...      employee   \n",
       "16  Have you permitted it to become a giveaway pro...        morale   \n",
       "17  Have you permitted it to become a giveaway pro...  consequently   \n",
       "18  Have you permitted it to become a giveaway pro...     increased   \n",
       "19  Have you permitted it to become a giveaway pro...  productivity   \n",
       "20                                        What effort        effort   \n",
       "21                            What effort do you make          make   \n",
       "22                  What effort do you make to assess        assess   \n",
       "23          What effort do you make to assess results       results   \n",
       "24  What effort do you make to assess results of y...       program   \n",
       "25                                     Do you measure       measure   \n",
       "26             Do you measure its relation to reduced       reduced   \n",
       "27  Do you measure its relation to reduced absente...   absenteeism   \n",
       "28  Do you measure its relation to reduced absente...      turnover   \n",
       "29  Do you measure its relation to reduced absente...     accidents   \n",
       "\n",
       "           lemma   pos     instance_id          labels  \n",
       "0           long   ADJ  d000.s000.t000  [bn:00106124a]  \n",
       "1             be  VERB  d000.s000.t001  [bn:00083181v]  \n",
       "2         review  VERB  d000.s000.t002  [bn:00092618v]  \n",
       "3      objective  NOUN  d000.s000.t003  [bn:00002179n]  \n",
       "4        benefit  NOUN  d000.s000.t004  [bn:00009904n]  \n",
       "5        service  NOUN  d000.s000.t005  [bn:00070654n]  \n",
       "6        program  NOUN  d000.s000.t006  [bn:00064646n]  \n",
       "7         permit  VERB  d000.s001.t000  [bn:00082536v]  \n",
       "8         become  VERB  d000.s001.t001  [bn:00083294v]  \n",
       "9       giveaway  NOUN  d000.s001.t002  [bn:00040564n]  \n",
       "10       program  NOUN  d000.s001.t003  [bn:00064646n]  \n",
       "11        rather   ADV  d000.s001.t004  [bn:00115972r]  \n",
       "12          have  VERB  d000.s001.t005  [bn:00088078v]  \n",
       "13          goal  NOUN  d000.s001.t006  [bn:00030721n]  \n",
       "14      improved   ADJ  d000.s001.t007  [bn:00104710a]  \n",
       "15      employee  NOUN  d000.s001.t008  [bn:00030618n]  \n",
       "16        morale  NOUN  d000.s001.t009  [bn:00055852n]  \n",
       "17  consequently   ADV  d000.s001.t010  [bn:00114106r]  \n",
       "18     increased   ADJ  d000.s001.t011  [bn:00104887a]  \n",
       "19  productivity  NOUN  d000.s001.t012  [bn:00064598n]  \n",
       "20        effort  NOUN  d000.s002.t000  [bn:00007011n]  \n",
       "21          make  VERB  d000.s002.t001  [bn:00087106v]  \n",
       "22        assess  VERB  d000.s002.t002  [bn:00082720v]  \n",
       "23        result  NOUN  d000.s002.t003  [bn:00034520n]  \n",
       "24       program  NOUN  d000.s002.t004  [bn:00064646n]  \n",
       "25       measure  VERB  d000.s003.t000  [bn:00082720v]  \n",
       "26       reduced   ADJ  d000.s003.t001  [bn:00101030a]  \n",
       "27   absenteeism  NOUN  d000.s003.t002  [bn:00000435n]  \n",
       "28      turnover  NOUN  d000.s003.t003  [bn:00030623n]  \n",
       "29      accident  NOUN  d000.s003.t004  [bn:00000690n]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = xml_to_dataframe(TRAIN_DATA_PATH, TRAIN_LABELS_PATH)\n",
    "train_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:50.567363Z",
     "iopub.status.busy": "2024-12-09T14:53:50.566723Z",
     "iopub.status.idle": "2024-12-09T14:53:50.716378Z",
     "shell.execute_reply": "2024-12-09T14:53:50.715555Z",
     "shell.execute_reply.started": "2024-12-09T14:53:50.567329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 226036 entries, 0 to 226035\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   sentence     226036 non-null  object\n",
      " 1   word         226036 non-null  object\n",
      " 2   lemma        226036 non-null  object\n",
      " 3   pos          226036 non-null  object\n",
      " 4   instance_id  226036 non-null  object\n",
      " 5   labels       226036 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:53.328220Z",
     "iopub.status.busy": "2024-12-09T14:53:53.327404Z",
     "iopub.status.idle": "2024-12-09T14:53:53.642486Z",
     "shell.execute_reply": "2024-12-09T14:53:53.641621Z",
     "shell.execute_reply.started": "2024-12-09T14:53:53.328184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The art</td>\n",
       "      <td>art</td>\n",
       "      <td>art</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>senseval2.d000.s000.t000</td>\n",
       "      <td>[bn:00005928n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The art of change-ringing</td>\n",
       "      <td>change-ringing</td>\n",
       "      <td>change_ringing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>senseval2.d000.s000.t001</td>\n",
       "      <td>[bn:00017671n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The art of change-ringing is peculiar</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>senseval2.d000.s000.t002</td>\n",
       "      <td>[bn:00108295a, bn:00108382a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The art of change-ringing is peculiar to the E...</td>\n",
       "      <td>English</td>\n",
       "      <td>english</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>senseval2.d000.s000.t003</td>\n",
       "      <td>[bn:00030863n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The art of change-ringing is peculiar to the E...</td>\n",
       "      <td>most</td>\n",
       "      <td>most</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>senseval2.d000.s000.t004</td>\n",
       "      <td>[bn:00106953a]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence            word  \\\n",
       "0                                            The art             art   \n",
       "1                          The art of change-ringing  change-ringing   \n",
       "2              The art of change-ringing is peculiar        peculiar   \n",
       "3  The art of change-ringing is peculiar to the E...         English   \n",
       "4  The art of change-ringing is peculiar to the E...            most   \n",
       "\n",
       "            lemma   pos               instance_id  \\\n",
       "0             art  NOUN  senseval2.d000.s000.t000   \n",
       "1  change_ringing  NOUN  senseval2.d000.s000.t001   \n",
       "2        peculiar   ADJ  senseval2.d000.s000.t002   \n",
       "3         english  NOUN  senseval2.d000.s000.t003   \n",
       "4            most   ADJ  senseval2.d000.s000.t004   \n",
       "\n",
       "                         labels  \n",
       "0                [bn:00005928n]  \n",
       "1                [bn:00017671n]  \n",
       "2  [bn:00108295a, bn:00108382a]  \n",
       "3                [bn:00030863n]  \n",
       "4                [bn:00106953a]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = xml_to_dataframe(TEST_DATA_PATH, TEST_LABELS_PATH)\n",
    "test_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:56.407347Z",
     "iopub.status.busy": "2024-12-09T14:53:56.406994Z",
     "iopub.status.idle": "2024-12-09T14:53:56.434797Z",
     "shell.execute_reply": "2024-12-09T14:53:56.433810Z",
     "shell.execute_reply.started": "2024-12-09T14:53:56.407317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your Oct. 6 editorial `` The Ill Homeless `` r...</td>\n",
       "      <td>referred</td>\n",
       "      <td>refer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>[bn:00082412v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Oct. 6 editorial `` The Ill Homeless `` r...</td>\n",
       "      <td>research</td>\n",
       "      <td>research</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>[bn:00067280n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your Oct. 6 editorial `` The Ill Homeless `` r...</td>\n",
       "      <td>reported</td>\n",
       "      <td>report</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>[bn:00092823v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your comments</td>\n",
       "      <td>comments</td>\n",
       "      <td>comment</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s001.t000</td>\n",
       "      <td>[bn:00020977n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your comments implied</td>\n",
       "      <td>implied</td>\n",
       "      <td>imply</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s001.t001</td>\n",
       "      <td>[bn:00085636v]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      word     lemma  \\\n",
       "0  Your Oct. 6 editorial `` The Ill Homeless `` r...  referred     refer   \n",
       "1  Your Oct. 6 editorial `` The Ill Homeless `` r...  research  research   \n",
       "2  Your Oct. 6 editorial `` The Ill Homeless `` r...  reported    report   \n",
       "3                                      Your comments  comments   comment   \n",
       "4                              Your comments implied   implied     imply   \n",
       "\n",
       "    pos     instance_id          labels  \n",
       "0  VERB  d000.s000.t000  [bn:00082412v]  \n",
       "1  NOUN  d000.s000.t001  [bn:00067280n]  \n",
       "2  VERB  d000.s000.t002  [bn:00092823v]  \n",
       "3  NOUN  d000.s001.t000  [bn:00020977n]  \n",
       "4  VERB  d000.s001.t001  [bn:00085636v]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = xml_to_dataframe(DEV_DATA_PATH, DEV_LABELS_PATH)\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:53:59.488644Z",
     "iopub.status.busy": "2024-12-09T14:53:59.488321Z",
     "iopub.status.idle": "2024-12-09T14:54:02.497674Z",
     "shell.execute_reply": "2024-12-09T14:54:02.496934Z",
     "shell.execute_reply.started": "2024-12-09T14:53:59.488616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it was full of rackets , balls and other objects</td>\n",
       "      <td>objects</td>\n",
       "      <td>object</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d0000001.s001.h001</td>\n",
       "      <td>[bn:00058442n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how big is that part compared to the whole</td>\n",
       "      <td>whole</td>\n",
       "      <td>whole</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d0000002.s001.h001</td>\n",
       "      <td>[bn:00079109n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the team is a unit</td>\n",
       "      <td>unit</td>\n",
       "      <td>unit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d0000003.s001.h001</td>\n",
       "      <td>[bn:00079109n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lard was also used , though its congener</td>\n",
       "      <td>congener</td>\n",
       "      <td>congener</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d0000004.s001.h001</td>\n",
       "      <td>[bn:00021806n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the American shopkeeper differs from his Europ...</td>\n",
       "      <td>congener</td>\n",
       "      <td>congener</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d0000005.s001.h001</td>\n",
       "      <td>[bn:00021806n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      word     lemma  \\\n",
       "0   it was full of rackets , balls and other objects   objects    object   \n",
       "1         how big is that part compared to the whole     whole     whole   \n",
       "2                                 the team is a unit      unit      unit   \n",
       "3           lard was also used , though its congener  congener  congener   \n",
       "4  the American shopkeeper differs from his Europ...  congener  congener   \n",
       "\n",
       "    pos         instance_id          labels  \n",
       "0  NOUN  d0000001.s001.h001  [bn:00058442n]  \n",
       "1  NOUN  d0000002.s001.h001  [bn:00079109n]  \n",
       "2  NOUN  d0000003.s001.h001  [bn:00079109n]  \n",
       "3  NOUN  d0000004.s001.h001  [bn:00021806n]  \n",
       "4  NOUN  d0000005.s001.h001  [bn:00021806n]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df = xml_to_dataframe(EXAMPLE_DATA_PATH, EXAMPLE_LABELS_PATH)\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:54:05.479348Z",
     "iopub.status.busy": "2024-12-09T14:54:05.478796Z",
     "iopub.status.idle": "2024-12-09T14:54:20.239291Z",
     "shell.execute_reply": "2024-12-09T14:54:20.238483Z",
     "shell.execute_reply.started": "2024-12-09T14:54:05.479315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entity</td>\n",
       "      <td>entity</td>\n",
       "      <td>entity</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d0000001.s001.h001</td>\n",
       "      <td>[bn:00031027n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entity : that which is perceived</td>\n",
       "      <td>perceived</td>\n",
       "      <td>perceive</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d0000001.s001.t002</td>\n",
       "      <td>[bn:00091540v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entity : that which is perceived or known</td>\n",
       "      <td>known</td>\n",
       "      <td>known</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d0000001.s001.t003</td>\n",
       "      <td>[bn:00105645a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entity : that which is perceived or known or i...</td>\n",
       "      <td>inferred</td>\n",
       "      <td>infer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d0000001.s001.t004</td>\n",
       "      <td>[bn:00086431v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entity : that which is perceived or known or i...</td>\n",
       "      <td>distinct</td>\n",
       "      <td>distinct</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d0000001.s001.t005</td>\n",
       "      <td>[bn:00099815a]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence       word     lemma  \\\n",
       "0                                             entity     entity    entity   \n",
       "1                   entity : that which is perceived  perceived  perceive   \n",
       "2          entity : that which is perceived or known      known     known   \n",
       "3  entity : that which is perceived or known or i...   inferred     infer   \n",
       "4  entity : that which is perceived or known or i...   distinct  distinct   \n",
       "\n",
       "    pos         instance_id          labels  \n",
       "0  NOUN  d0000001.s001.h001  [bn:00031027n]  \n",
       "1  VERB  d0000001.s001.t002  [bn:00091540v]  \n",
       "2   ADJ  d0000001.s001.t003  [bn:00105645a]  \n",
       "3  VERB  d0000001.s001.t004  [bn:00086431v]  \n",
       "4   ADJ  d0000001.s001.t005  [bn:00099815a]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glosses_df = xml_to_dataframe(GLOSSES_DATA_PATH, GLOSSES_LABELS_PATH)\n",
    "glosses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:54:23.622691Z",
     "iopub.status.busy": "2024-12-09T14:54:23.621898Z",
     "iopub.status.idle": "2024-12-09T14:54:23.627259Z",
     "shell.execute_reply": "2024-12-09T14:54:23.626452Z",
     "shell.execute_reply.started": "2024-12-09T14:54:23.622656Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples dans l'entraînement : 226036\n",
      "Nombre d'exemples dans le test : 8062\n",
      "Nombre d'exemples dans le dev : 455\n",
      "Nombre d'exemples dans le example : 47825\n",
      "Nombre d'exemples dans le glosses : 566610\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre d'exemples dans l'entraînement : {len(train_df)}\")\n",
    "print(f\"Nombre d'exemples dans le test : {len(test_df)}\")\n",
    "print(f\"Nombre d'exemples dans le dev : {len(dev_df)}\")\n",
    "print(f\"Nombre d'exemples dans le example : {len(example_df)}\")\n",
    "print(f\"Nombre d'exemples dans le glosses : {len(glosses_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:54:26.679847Z",
     "iopub.status.busy": "2024-12-09T14:54:26.679522Z",
     "iopub.status.idle": "2024-12-09T14:54:26.696983Z",
     "shell.execute_reply": "2024-12-09T14:54:26.696017Z",
     "shell.execute_reply.started": "2024-12-09T14:54:26.679821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42174</th>\n",
       "      <td>Other classes are included only by myself ( in...</td>\n",
       "      <td>only</td>\n",
       "      <td>only</td>\n",
       "      <td>ADV</td>\n",
       "      <td>d042.s011.t006</td>\n",
       "      <td>[bn:00114234r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106459</th>\n",
       "      <td>We first define a function b ( t ) as follows</td>\n",
       "      <td>as follows</td>\n",
       "      <td>as_follows</td>\n",
       "      <td>ADV</td>\n",
       "      <td>d106.s088.t003</td>\n",
       "      <td>[bn:00114359r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172118</th>\n",
       "      <td>The specific staining by both direct and indir...</td>\n",
       "      <td>pseudophloem</td>\n",
       "      <td>pseudophloem</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d172.s074.t008</td>\n",
       "      <td>[bn:00064987n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91565</th>\n",
       "      <td>SBA works closely with the principal property ...</td>\n",
       "      <td>property</td>\n",
       "      <td>property</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d092.s025.t011</td>\n",
       "      <td>[bn:00009815n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50748</th>\n",
       "      <td>What one actually</td>\n",
       "      <td>actually</td>\n",
       "      <td>actually</td>\n",
       "      <td>ADV</td>\n",
       "      <td>d051.s008.t000</td>\n",
       "      <td>[bn:00114117r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104643</th>\n",
       "      <td>They indicated that a 4 - day retention , aera...</td>\n",
       "      <td>lagoon</td>\n",
       "      <td>lagoon</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d105.s007.t005</td>\n",
       "      <td>[bn:00049696n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99654</th>\n",
       "      <td>Remember that in seeking the modern in Utopia ...</td>\n",
       "      <td>not</td>\n",
       "      <td>not</td>\n",
       "      <td>ADV</td>\n",
       "      <td>d100.s020.t004</td>\n",
       "      <td>[bn:00116360r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211738</th>\n",
       "      <td>Somehow our contemporary Moloch must be induce...</td>\n",
       "      <td>see</td>\n",
       "      <td>see</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d295.s023.t001</td>\n",
       "      <td>[bn:00092443v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132691</th>\n",
       "      <td>Since * * f and p divides</td>\n",
       "      <td>divides</td>\n",
       "      <td>divide</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d133.s030.t000</td>\n",
       "      <td>[bn:00087098v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65199</th>\n",
       "      <td>Even so , he generally listened and was usuall...</td>\n",
       "      <td>reasonable</td>\n",
       "      <td>reasonable</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d065.s084.t005</td>\n",
       "      <td>[bn:00109507a]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence          word  \\\n",
       "42174   Other classes are included only by myself ( in...          only   \n",
       "106459      We first define a function b ( t ) as follows    as follows   \n",
       "172118  The specific staining by both direct and indir...  pseudophloem   \n",
       "91565   SBA works closely with the principal property ...      property   \n",
       "50748                                   What one actually      actually   \n",
       "104643  They indicated that a 4 - day retention , aera...        lagoon   \n",
       "99654   Remember that in seeking the modern in Utopia ...           not   \n",
       "211738  Somehow our contemporary Moloch must be induce...           see   \n",
       "132691                          Since * * f and p divides       divides   \n",
       "65199   Even so , he generally listened and was usuall...    reasonable   \n",
       "\n",
       "               lemma   pos     instance_id          labels  \n",
       "42174           only   ADV  d042.s011.t006  [bn:00114234r]  \n",
       "106459    as_follows   ADV  d106.s088.t003  [bn:00114359r]  \n",
       "172118  pseudophloem  NOUN  d172.s074.t008  [bn:00064987n]  \n",
       "91565       property  NOUN  d092.s025.t011  [bn:00009815n]  \n",
       "50748       actually   ADV  d051.s008.t000  [bn:00114117r]  \n",
       "104643        lagoon  NOUN  d105.s007.t005  [bn:00049696n]  \n",
       "99654            not   ADV  d100.s020.t004  [bn:00116360r]  \n",
       "211738           see  VERB  d295.s023.t001  [bn:00092443v]  \n",
       "132691        divide  VERB  d133.s030.t000  [bn:00087098v]  \n",
       "65199     reasonable   ADJ  d065.s084.t005  [bn:00109507a]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:54:29.413303Z",
     "iopub.status.busy": "2024-12-09T14:54:29.412979Z",
     "iopub.status.idle": "2024-12-09T14:54:29.568259Z",
     "shell.execute_reply": "2024-12-09T14:54:29.567404Z",
     "shell.execute_reply.started": "2024-12-09T14:54:29.413277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 226036 entries, 0 to 226035\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   sentence     226036 non-null  object\n",
      " 1   word         226036 non-null  object\n",
      " 2   lemma        226036 non-null  object\n",
      " 3   pos          226036 non-null  object\n",
      " 4   instance_id  226036 non-null  object\n",
      " 5   labels       226036 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 10.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How long</td>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>[bn:00106124a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long has it been</td>\n",
       "      <td>been</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>[bn:00083181v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How long has it been since you reviewed</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>review</td>\n",
       "      <td>VERB</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>[bn:00092618v]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>objectives</td>\n",
       "      <td>objective</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>[bn:00002179n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit</td>\n",
       "      <td>benefit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>[bn:00009904n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence        word      lemma  \\\n",
       "0                                           How long        long       long   \n",
       "1                               How long has it been        been         be   \n",
       "2            How long has it been since you reviewed    reviewed     review   \n",
       "3  How long has it been since you reviewed the ob...  objectives  objective   \n",
       "4  How long has it been since you reviewed the ob...     benefit    benefit   \n",
       "\n",
       "    pos     instance_id          labels  \n",
       "0   ADJ  d000.s000.t000  [bn:00106124a]  \n",
       "1  VERB  d000.s000.t001  [bn:00083181v]  \n",
       "2  VERB  d000.s000.t002  [bn:00092618v]  \n",
       "3  NOUN  d000.s000.t003  [bn:00002179n]  \n",
       "4  NOUN  d000.s000.t004  [bn:00009904n]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:57:00.842983Z",
     "iopub.status.busy": "2024-12-09T14:57:00.842579Z",
     "iopub.status.idle": "2024-12-09T14:57:00.847992Z",
     "shell.execute_reply": "2024-12-09T14:57:00.846978Z",
     "shell.execute_reply.started": "2024-12-09T14:57:00.842949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:54:55.620063Z",
     "iopub.status.busy": "2024-12-09T14:54:55.619100Z",
     "iopub.status.idle": "2024-12-09T14:54:55.627022Z",
     "shell.execute_reply": "2024-12-09T14:54:55.625692Z",
     "shell.execute_reply.started": "2024-12-09T14:54:55.620023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WSDDataset(Dataset):\n",
    "    def __init__(self, sentences, words, labels, tokenizer, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word = self.words[idx]\n",
    "        \n",
    "        # Mise en évidence du mot à désambiguïser\n",
    "        highlighted_sentence = sentence.replace(word, f\"[{word}]\")\n",
    "        \n",
    "        # Encodage avec RoBERTa\n",
    "        encoding = self.tokenizer(\n",
    "            highlighted_sentence, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:55:03.328150Z",
     "iopub.status.busy": "2024-12-09T14:55:03.327194Z",
     "iopub.status.idle": "2024-12-09T14:55:03.332442Z",
     "shell.execute_reply": "2024-12-09T14:55:03.331372Z",
     "shell.execute_reply.started": "2024-12-09T14:55:03.328096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_wsd_model(num_labels):\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:55:06.791982Z",
     "iopub.status.busy": "2024-12-09T14:55:06.790926Z",
     "iopub.status.idle": "2024-12-09T14:55:06.796465Z",
     "shell.execute_reply": "2024-12-09T14:55:06.795496Z",
     "shell.execute_reply.started": "2024-12-09T14:55:06.791940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_previous_checkpoints(checkpoint_dir):\n",
    "    \"\"\"Supprime les anciens fichiers de checkpoint.\"\"\"\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        for file in os.listdir(checkpoint_dir):\n",
    "            file_path = os.path.join(checkpoint_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:54:45.100114Z",
     "iopub.status.busy": "2024-12-09T13:54:45.099790Z",
     "iopub.status.idle": "2024-12-09T13:54:48.369646Z",
     "shell.execute_reply": "2024-12-09T13:54:48.368284Z",
     "shell.execute_reply.started": "2024-12-09T13:54:45.100087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "! rm -r /kaggle/working/checkpoints\n",
    "! rm -r /kaggle/working/download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:55:18.291149Z",
     "iopub.status.busy": "2024-12-09T14:55:18.290404Z",
     "iopub.status.idle": "2024-12-09T14:55:18.304546Z",
     "shell.execute_reply": "2024-12-09T14:55:18.303345Z",
     "shell.execute_reply.started": "2024-12-09T14:55:18.291086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_wsd_model(\n",
    "    model, train_dataloader, val_dataloader, device, \n",
    "    tokenizer, epochs=10, checkpoint_dir='/kaggle/working/checkpoints', download_dir='/kaggle/working/download_dir', MAX_CHECKPOINTS=3\n",
    "):\n",
    "    import shutil  # Pour la suppression des anciens checkpoints\n",
    "    from collections import deque  # Pour gérer un historique des checkpoints\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    global_step = 0  # Nombre de pas\n",
    "    saved_checkpoints = deque()  # Historique des checkpoints\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Utilisation de tqdm pour la barre de progression\n",
    "        train_loop = tqdm(train_dataloader, desc=f\"Époque {epoch+1}/{epochs} - Entraînement\")\n",
    "        for batch in train_loop:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Mise à jour de la barre de progression\n",
    "            train_loop.set_postfix(Loss=loss.item())\n",
    "\n",
    "            # Sauvegarde périodique\n",
    "            global_step += 1\n",
    "            if global_step % 3000 == 0:\n",
    "                checkpoint_name = f\"checkpoint_epoch_{epoch}_step_{global_step}\"\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "                model.save_pretrained(checkpoint_path)\n",
    "                tokenizer.save_pretrained(checkpoint_path)\n",
    "                print(f\"Checkpoint sauvegardé : {checkpoint_name}\")\n",
    "                \n",
    "                # Copier dans le répertoire de téléchargement\n",
    "                download_checkpoint_path = os.path.join(download_dir, checkpoint_name)\n",
    "                shutil.copytree(checkpoint_path, download_checkpoint_path)\n",
    "                print(f\"Checkpoint copié dans : {download_checkpoint_path}\")\n",
    "\n",
    "                # Ajouter le checkpoint au suivi\n",
    "                saved_checkpoints.append(download_checkpoint_path)\n",
    "                \n",
    "                # Supprimer les anciens checkpoints si MAX_CHECKPOINTS est dépassé\n",
    "                if len(saved_checkpoints) > MAX_CHECKPOINTS:\n",
    "                    oldest_checkpoint = saved_checkpoints.popleft()\n",
    "                    shutil.rmtree(oldest_checkpoint, ignore_errors=True)\n",
    "                    print(f\"Ancien checkpoint supprimé : {oldest_checkpoint}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        val_loop = tqdm(val_dataloader, desc=f\"Époque {epoch+1}/{epochs} - Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loop:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_val_loss += outputs.loss.item()\n",
    "                val_loop.set_postfix(Val_Loss=outputs.loss.item())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Validation/Loss', avg_val_loss, epoch)\n",
    "\n",
    "        print(f\"Époque {epoch+1}/{epochs}\")\n",
    "        print(f\"Perte d'entraînement : {avg_train_loss:.4f}\")\n",
    "        print(f\"Perte de validation : {avg_val_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:57:12.755242Z",
     "iopub.status.busy": "2024-12-09T14:57:12.754533Z",
     "iopub.status.idle": "2024-12-09T14:57:12.765360Z",
     "shell.execute_reply": "2024-12-09T14:57:12.764380Z",
     "shell.execute_reply.started": "2024-12-09T14:57:12.755204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(train_df):\n",
    "    # Convertir les labels de liste à chaîne de caractères\n",
    "    def clean_label(x):\n",
    "        if isinstance(x, list):\n",
    "            return x[0].strip('[]')\n",
    "        return x.strip('[]')\n",
    "    \n",
    "    train_df['cleaned_labels'] = train_df['labels'].apply(clean_label)\n",
    "    \n",
    "    # Encoder les labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df['encoded_labels'] = label_encoder.fit_transform(train_df['cleaned_labels'])\n",
    "    \n",
    "    # Préparation des données\n",
    "    X = train_df[['sentence', 'word']]\n",
    "    y = train_df['encoded_labels']\n",
    "    \n",
    "    # Division des données\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Configuration du dispositif\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Préparation du modèle\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    model, tokenizer = prepare_wsd_model(num_labels)\n",
    "    \n",
    "    # Création des datasets\n",
    "    max_length = 128\n",
    "    train_dataset = WSDDataset(\n",
    "        X_train['sentence'].values, \n",
    "        X_train['word'].values, \n",
    "        y_train.values, \n",
    "        tokenizer, \n",
    "        max_length\n",
    "    )\n",
    "    val_dataset = WSDDataset(\n",
    "        X_val['sentence'].values, \n",
    "        X_val['word'].values, \n",
    "        y_val.values, \n",
    "        tokenizer, \n",
    "        max_length\n",
    "    )\n",
    "    \n",
    "    # Création des dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    trained_model = train_wsd_model(model, train_dataloader, val_dataloader, device, tokenizer)\n",
    "    \n",
    "    # Sauvegarde finale\n",
    "    trained_model.save_pretrained('/kaggle/working/wsd_roberta_model')\n",
    "    tokenizer.save_pretrained('/kaggle/working/wsd_roberta_tokenizer')\n",
    "    \n",
    "    # Mapping des labels originaux\n",
    "    label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "    \n",
    "    # Répertoire où sauvegarder le fichier\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Crée le répertoire s'il n'existe pas\n",
    "    \n",
    "    # Chemin complet pour le fichier\n",
    "    output_file = os.path.join(output_dir, \"label_mapping.json\")\n",
    "    \n",
    "    # Sauvegarde du mapping\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(label_mapping, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\nMapping des labels sauvegardé dans '{output_file}'.\")\n",
    "    \n",
    "    # Affichage limité des labels (facultatif)\n",
    "    print(\"\\nVérification du mapping des labels (affichage des 10 premiers) :\")\n",
    "    for encoded, original in list(label_mapping.items())[:10]:\n",
    "        print(f\"{encoded}: {original}\")\n",
    "\n",
    "    \n",
    "    return label_encoder, trained_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T14:57:17.430370Z",
     "iopub.status.busy": "2024-12-09T14:57:17.429767Z",
     "iopub.status.idle": "2024-12-09T16:46:47.865192Z",
     "shell.execute_reply": "2024-12-09T16:46:47.863319Z",
     "shell.execute_reply.started": "2024-12-09T14:57:17.430333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Époque 1/10 - Entraînement:  27%|██▋       | 2999/11302 [44:06<2:04:36,  1.11it/s, Loss=8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint sauvegardé : checkpoint_epoch_0_step_3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 1/10 - Entraînement:  27%|██▋       | 3000/11302 [44:08<3:21:55,  1.46s/it, Loss=8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint copié dans : /kaggle/working/download_dir/checkpoint_epoch_0_step_3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 1/10 - Entraînement:  53%|█████▎    | 5999/11302 [1:28:27<1:15:19,  1.17it/s, Loss=5.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint sauvegardé : checkpoint_epoch_0_step_6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 1/10 - Entraînement:  53%|█████▎    | 6000/11302 [1:28:29<2:03:22,  1.40s/it, Loss=5.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint copié dans : /kaggle/working/download_dir/checkpoint_epoch_0_step_6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 1/10 - Entraînement:  66%|██████▌   | 7416/11302 [1:49:23<57:19,  1.13it/s, Loss=7.91]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Utilisation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m label_encoder, model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Optionnel : Vérification du nombre de labels uniques\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre de labels uniques : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train_df)\u001b[0m\n\u001b[1;32m     51\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_wsd_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Sauvegarde finale\u001b[39;00m\n\u001b[1;32m     57\u001b[0m trained_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/wsd_roberta_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mtrain_wsd_model\u001b[0;34m(model, train_dataloader, val_dataloader, device, tokenizer, epochs, checkpoint_dir, download_dir, MAX_CHECKPOINTS)\u001b[0m\n\u001b[1;32m     28\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     38\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1318\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1318\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1330\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         output_attentions,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:574\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:472\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 472\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Utilisation\n",
    "label_encoder, model, tokenizer = main(train_df)\n",
    "\n",
    "# Optionnel : Vérification du nombre de labels uniques\n",
    "print(f\"Nombre de labels uniques : {len(label_encoder.classes_)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6258848,
     "sourceId": 10140693,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
