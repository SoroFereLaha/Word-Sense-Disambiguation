{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2asL7LVQqNvF",
        "outputId": "ebd4735d-95d3-451e-c390-a6f1d889aa99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from xml.etree.ElementTree import ElementTree\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# Téléchargement de WordNet si nécessaire\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes pour WordNet\n",
        "WORDNET_POS = {'VERB': wn.VERB, 'NOUN': wn.NOUN, 'ADJ': wn.ADJ, 'ADV': wn.ADV}\n",
        "\n",
        "# Constantes globales\n",
        "HEADERS = ['id', 'sentence', 'sense_keys', 'glosses', 'targets']\n",
        "TGT_TOKEN = '[TGT]'\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Paramètres utilisateur à définir directement\n",
        "corpus_dir = \"/content/corpus_dir\"  # Répertoire contenant les fichiers XML et TXT\n",
        "output_dir = \"/content/output_dir\"  # Répertoire où enregistrer le fichier CSV\n",
        "max_num_gloss = 5  # Limite max des gloses (mettre une valeur ou None pour aucune limite)\n",
        "use_augmentation = True  # Activer l'augmentation des données avec WordNet"
      ],
      "metadata": {
        "id": "wXpBqGKRumvD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_info(lemma, pos, info_type):\n",
        "    \"\"\"\n",
        "    Récupère les informations de WordNet pour un lemme donné, comme les gloses ou les exemples.\n",
        "    \"\"\"\n",
        "    results = dict()\n",
        "    wn_pos = WORDNET_POS[pos] if pos is not None else None\n",
        "    morphemes = wn._morphy(lemma, pos=wn_pos) if pos is not None else []\n",
        "    for i, synset in enumerate(set(wn.synsets(lemma, pos=wn_pos))):\n",
        "        sense_key = None\n",
        "        for l in synset.lemmas():\n",
        "            if l.name().lower() == lemma.lower():\n",
        "                sense_key = l.key()\n",
        "                break\n",
        "            elif l.name().lower() in morphemes:\n",
        "                sense_key = l.key()\n",
        "        assert sense_key is not None\n",
        "        results[sense_key] = synset.examples() if info_type == 'examples' else synset.definition()\n",
        "    return results\n",
        "\n",
        "def get_glosses(lemma, pos):\n",
        "    \"\"\"\n",
        "    Récupère les gloses (définitions) associées à un lemme et une catégorie grammaticale.\n",
        "    \"\"\"\n",
        "    return _get_info(lemma, pos, info_type='gloss')\n",
        "\n",
        "def get_example_sentences(lemma, pos):\n",
        "    \"\"\"\n",
        "    Récupère les phrases d'exemple associées à un lemme et une catégorie grammaticale.\n",
        "    \"\"\"\n",
        "    return _get_info(lemma, pos, info_type='examples')\n",
        "\n",
        "def get_all_wordnet_lemma_names():\n",
        "    \"\"\"\n",
        "    Récupère tous les lemmes de WordNet classés par catégorie grammaticale.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for pos, wn_pos in WORDNET_POS.items():\n",
        "        results.append((pos, wn.all_lemma_names(pos=wn_pos)))\n",
        "    return results"
      ],
      "metadata": {
        "id": "GbSjs5EutN5B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_csv():\n",
        "    \"\"\"\n",
        "    Génère un fichier CSV à partir d'un corpus XML/WordNet pour une tâche de sélection de gloses.\n",
        "    \"\"\"\n",
        "    corpus_dir_path = Path(corpus_dir)\n",
        "    corpus_name = corpus_dir_path.name.lower()\n",
        "    xml_path = str(corpus_dir_path.joinpath(f\"{corpus_name}.data.xml\"))\n",
        "    txt_path = str(corpus_dir_path.joinpath(f\"{corpus_name}.gold.key.txt\"))\n",
        "    output_filename = f\"{corpus_name}\"\n",
        "    if max_num_gloss:\n",
        "        output_filename += f\"-max_num_gloss={max_num_gloss}\"\n",
        "    if use_augmentation:\n",
        "        output_filename += \"-augmented\"\n",
        "    csv_path = str(Path(output_dir).joinpath(f\"{output_filename}.csv\"))\n",
        "\n",
        "    print(\"Création des données pour la tâche de sélection de gloses...\")\n",
        "    record_count = 0\n",
        "    gloss_count = 0\n",
        "    max_gloss_count = 0\n",
        "\n",
        "    # Traitement du fichier XML et génération du CSV\n",
        "    xml_root = ElementTree(file=xml_path).getroot()\n",
        "    with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
        "        csv_writer = csv.writer(f)\n",
        "        csv_writer.writerow(HEADERS)\n",
        "\n",
        "        def _write_to_csv(_id, _sentence, _lemma, _pos, _gold_keys):\n",
        "            \"\"\"\n",
        "            Écrit une ligne dans le fichier CSV.\n",
        "            \"\"\"\n",
        "            nonlocal record_count, gloss_count, max_gloss_count\n",
        "\n",
        "            # Récupération des gloses pour le lemme donné\n",
        "            sense_info = get_glosses(_lemma, _pos)\n",
        "            if max_num_gloss is not None:\n",
        "                sense_gloss_pairs = []\n",
        "                for k in _gold_keys:\n",
        "                    sense_gloss_pairs.append((k, sense_info[k]))\n",
        "                    del sense_info[k]\n",
        "\n",
        "                remainder = max_num_gloss - len(sense_gloss_pairs)\n",
        "                if len(sense_info) > remainder:\n",
        "                    for p in random.sample(list(sense_info.items()), remainder):\n",
        "                        sense_gloss_pairs.append(p)\n",
        "                elif len(sense_info) > 0:\n",
        "                    sense_gloss_pairs += list(sense_info.items())\n",
        "\n",
        "                random.shuffle(sense_gloss_pairs)\n",
        "                sense_keys, glosses = zip(*sense_gloss_pairs)\n",
        "            else:\n",
        "                sense_keys, glosses = zip(*sense_info.items())\n",
        "\n",
        "            # Création des cibles et écriture dans le CSV\n",
        "            targets = [sense_keys.index(k) for k in _gold_keys]\n",
        "            csv_writer.writerow([_id, _sentence, list(sense_keys), list(glosses), targets])\n",
        "\n",
        "            record_count += 1\n",
        "            gloss_count += len(glosses)\n",
        "            max_gloss_count = max(max_gloss_count, len(glosses))\n",
        "\n",
        "        with open(txt_path, 'r', encoding='utf-8') as g:\n",
        "            for doc in tqdm(xml_root):\n",
        "                for sent in doc:\n",
        "                    tokens = []\n",
        "                    instances = []\n",
        "                    for token in sent:\n",
        "                        tokens.append(token.text)\n",
        "                        if token.tag == 'instance':\n",
        "                            start_idx = len(tokens) - 1\n",
        "                            end_idx = start_idx + 1\n",
        "                            instances.append((token.attrib['id'], start_idx, end_idx, token.attrib['lemma'], token.attrib['pos']))\n",
        "\n",
        "                    for id_, start, end, lemma, pos in instances:\n",
        "                        gold = g.readline().strip().split()\n",
        "                        gold_keys = gold[1:]\n",
        "                        assert id_ == gold[0]\n",
        "\n",
        "                        sentence = \" \".join(\n",
        "                            tokens[:start] + [TGT_TOKEN] + tokens[start:end] + [TGT_TOKEN] + tokens[end:]\n",
        "                        )\n",
        "                        _write_to_csv(id_, sentence, lemma, pos, gold_keys)\n",
        "\n",
        "        if use_augmentation:\n",
        "            print(\"Création de données supplémentaires en utilisant les phrases d'exemple de WordNet...\")\n",
        "            counter = 0\n",
        "            for pos, lemma_name_generator in get_all_wordnet_lemma_names():\n",
        "                print(f\"Traitement de {pos}...\")\n",
        "                for lemma in tqdm(list(lemma_name_generator)):\n",
        "                    for gold_key, examples in get_example_sentences(lemma, pos).items():\n",
        "                        for example_sentence in examples:\n",
        "                            re_result = re.search(rf\"\\b{lemma.lower()}\\b\", example_sentence.lower())\n",
        "                            if re_result is not None:\n",
        "                                start, end = re_result.span()\n",
        "                                sentence = f\"{example_sentence[:start]}\" \\\n",
        "                                    f\"{TGT_TOKEN} {example_sentence[start:end]} {TGT_TOKEN}\" \\\n",
        "                                    f\"{example_sentence[end:]}\".strip()\n",
        "                                _write_to_csv(f\"wn-aug-{counter}\", sentence, lemma, pos, [gold_key])\n",
        "                                counter += 1\n",
        "\n",
        "    print(\n",
        "        f\"Terminé.\\n\"\n",
        "        f\"Nombre d'enregistrements : {record_count}\\n\"\n",
        "        f\"Nombre moyen de gloses par enregistrement : {gloss_count / record_count:.2f}\\n\"\n",
        "        f\"Nombre maximum de gloses dans un enregistrement : {max_gloss_count}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "NqQxazyGtAt0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exécution de la fonction\n",
        "generate_csv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XHCfM8XtgY2",
        "outputId": "1afec659-97f3-491c-946f-2de0e659c81a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création des données pour la tâche de sélection de gloses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/352 [00:00<?, ?it/s]<ipython-input-4-44d56ced9f8c>:43: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  for p in random.sample(sense_info.items(), remainder):\n",
            "100%|██████████| 352/352 [00:21<00:00, 16.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création de données supplémentaires en utilisant les phrases d'exemple de WordNet...\n",
            "Traitement de VERB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11529/11529 [00:01<00:00, 7551.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traitement de NOUN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117798/117798 [00:09<00:00, 12687.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traitement de ADJ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21479/21479 [00:02<00:00, 7836.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traitement de ADV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4481/4481 [00:00<00:00, 7750.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terminé.\n",
            "Nombre d'enregistrements : 263632\n",
            "Nombre moyen de gloses par enregistrement : 3.51\n",
            "Nombre maximum de gloses dans un enregistrement : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exécution de la fonction\n",
        "generate_csv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyYNPTMO9Krb",
        "outputId": "20e24123-da1a-4e33-bbf6-9c6609636571"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création des données pour la tâche de sélection de gloses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 72.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création de données supplémentaires en utilisant les phrases d'exemple de WordNet...\n",
            "Traitement de VERB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11529/11529 [00:01<00:00, 9981.91it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traitement de NOUN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117798/117798 [00:04<00:00, 25093.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traitement de ADJ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21479/21479 [00:02<00:00, 9696.96it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traitement de ADV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4481/4481 [00:00<00:00, 10095.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terminé.\n",
            "Nombre d'enregistrements : 38051\n",
            "Nombre moyen de gloses par enregistrement : 2.86\n",
            "Nombre maximum de gloses dans un enregistrement : 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}